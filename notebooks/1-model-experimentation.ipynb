{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "sys.path.append('../model')\n",
    "\n",
    "import config\n",
    "from game import *\n",
    "from tileset import *\n",
    "from mdlstm import *\n",
    "\n",
    "from time import time\n",
    "\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека изчетем данните. Измежду тях ще изберем само карти за двама играчи, с най-малкия възможен размер (64 X 64), и използващи плочки тип `jungle`. Oчакваме картите да са 11 на брой. Бройката е доста малка, но ще разчитаме на [неразумната ефективност на рекурентните невронни мрежи](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios = []\n",
    "game = Game(config.STARCRAFT_ROOT)\n",
    "game.process_game_scenarios()\n",
    "for directory in config.MAP_DIRECTORIES:\n",
    "    scenarios += game.process_scenarios(directory)\n",
    "\n",
    "scenarios = [x for x in scenarios if x.alliances == x.human_players == 2 and x.tileset == Tileset.JUNGLE and x.width == x.height == 64]\n",
    "\n",
    "len(scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ще имплементираме помощни функции, които извличат feature-и от плочка. Плочките силно зависят от околните. И може би има смисъл да приложим нещо подобно на word2vec.\n",
    "\n",
    "Но за сега ще направим малко feature engineering, за да изпробваме модела. Ще работим с 3 feature-а:\n",
    "- осреднена височина на плочката\n",
    "- коефициент за това върху каква част от плочката може да се ходи\n",
    "- дали върху плочката може да се строи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 64\n",
    "w = 64\n",
    "\n",
    "tile_vec_size = 3\n",
    "input_vec_size = 2 * tile_vec_size\n",
    "output_vec_size = tile_vec_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def to_features(tile):\n",
    "\n",
    "    @np.vectorize\n",
    "    def minitile_heights(minitile):\n",
    "        return minitile.height\n",
    "\n",
    "    @np.vectorize\n",
    "    def minitile_walkability(minitile):\n",
    "        return minitile.walkable\n",
    "\n",
    "    return np.array([\n",
    "        np.average(minitile_heights(tile.minitiles)),\n",
    "        np.average(minitile_walkability(tile.minitiles)),\n",
    "        tile.buildable\n",
    "    ], dtype=np.float32) + 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_features_by_index(tiles, horizontal_index, vertical_index):\n",
    "    if vertical_index >= 0 and horizontal_index >= 0:\n",
    "        tile = tiles[vertical_index, horizontal_index]\n",
    "        return to_features(tile)\n",
    "    else:\n",
    "        return np.zeros([tile_vec_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Експлицитни feature-и на всяка стъпка\n",
    "\n",
    "Всяка плочка ще зависи от горната и от лявата.\n",
    "\n",
    "В нашата двуизмерна рекурентна невронна мрежа ще използваме конкатенацията на feature-ите на горната и лявата плочки за вход. За изход ще използваме feature-ите на текущата плочка. За feature-и на плочки извън игралното поле ще връщаме списък нули.\n",
    "\n",
    "Всъщност скритият state за RNN-а също идва от горната и от лявата плочка. Така че на теория би трябвало да не е нужно да ги даваме експлицитно. Но при по-ранни експерименти имах **огромни** проблеми с числената стабилност на модела. За всякакъв смислено голям learning rate получавах NaN-ове в loss-a. Което всъщност не е изненада. Дори е описано в документацията на модела. Експлицитните feature-и са трик, който се справя с този проблем.\n",
    "\n",
    "Експлицитните feature-и на всяка стъпка имат още един плюс. Когато използваме модела за генерация, ще можем да избираме плочка различна от най-вероятната за всяка от стъпките."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    x = np.empty((h, w, input_vec_size), dtype=np.float32)\n",
    "    y = np.empty((h, w, output_vec_size), dtype=np.float32)\n",
    "    for vertical_index in range(h):\n",
    "        for horizontal_index in range(w):\n",
    "            top_tile_features = to_features_by_index(scenario.tiles, vertical_index - 1, horizontal_index)\n",
    "            left_tile_features = to_features_by_index(scenario.tiles, vertical_index, horizontal_index - 1)\n",
    "            x[vertical_index, horizontal_index, :] = np.concatenate([top_tile_features, left_tile_features])\n",
    "            y[vertical_index, horizontal_index, :] = to_features_by_index(scenario.tiles, vertical_index, horizontal_index)\n",
    "    data.append((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(data, batch_size, epochs):\n",
    "    all_batches = []\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(data)\n",
    "        all_batches += data\n",
    "\n",
    "    for i in range(0, epochs * len(data), batch_size):\n",
    "        inputs = all_batches[i: i + batch_size]\n",
    "        if len(inputs) == batch_size:\n",
    "            yield inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Терниране на модела\n",
    "\n",
    "Моделът идващ от модула `mdlstm`, както и кодът в тази тетрадка се базира на [tensorflow-multi-dimensional-lstm](https://github.com/philipperemy/tensorflow-multi-dimensional-lstm) написан от [Philippe Rémy](https://github.com/philipperemy) с разни промени от моя страна.\n",
    "\n",
    "Нека на базата на този код да имплементираме примерен трениращ код и да проверим дали моделът конвергира."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.24446\n",
      "steps = 0010 | overall loss = 0.18422\n",
      "steps = 0020 | overall loss = 0.15899\n",
      "steps = 0030 | overall loss = 0.16026\n",
      "steps = 0040 | overall loss = 0.14555\n",
      "steps = 0050 | overall loss = 0.14163\n",
      "steps = 0060 | overall loss = 0.14052\n",
      "steps = 0070 | overall loss = 0.13060\n",
      "steps = 0080 | overall loss = 0.11840\n",
      "steps = 0090 | overall loss = 0.10667\n",
      "steps = 0100 | overall loss = 0.09992\n",
      "steps = 0110 | overall loss = 0.09374\n",
      "steps = 0120 | overall loss = 0.08738\n",
      "steps = 0130 | overall loss = 0.08464\n",
      "steps = 0140 | overall loss = 0.07915\n",
      "steps = 0150 | overall loss = 0.07158\n",
      "steps = 0160 | overall loss = 0.06686\n",
      "steps = 0170 | overall loss = 0.06214\n",
      "steps = 0180 | overall loss = 0.05708\n",
      "steps = 0190 | overall loss = 0.05526\n",
      "steps = 0200 | overall loss = 0.05008\n",
      "steps = 0210 | overall loss = 0.04769\n",
      "steps = 0220 | overall loss = 0.04510\n",
      "steps = 0230 | overall loss = 0.04351\n",
      "steps = 0240 | overall loss = 0.04303\n",
      "steps = 0250 | overall loss = 0.03690\n",
      "steps = 0260 | overall loss = 0.03764\n",
      "steps = 0270 | overall loss = 0.03685\n",
      "steps = 0280 | overall loss = 0.03862\n",
      "steps = 0290 | overall loss = 0.03382\n",
      "steps = 0300 | overall loss = 0.03425\n",
      "steps = 0310 | overall loss = 0.03165\n",
      "steps = 0320 | overall loss = 0.03179\n",
      "steps = 0330 | overall loss = 0.03295\n",
      "steps = 0340 | overall loss = 0.03044\n",
      "steps = 0350 | overall loss = 0.02898\n",
      "steps = 0360 | overall loss = 0.03116\n",
      "steps = 0370 | overall loss = 0.02938\n",
      "steps = 0380 | overall loss = 0.02953\n",
      "steps = 0390 | overall loss = 0.02702\n",
      "steps = 0400 | overall loss = 0.02722\n",
      "steps = 0410 | overall loss = 0.02592\n",
      "steps = 0420 | overall loss = 0.03098\n",
      "steps = 0430 | overall loss = 0.02729\n",
      "steps = 0440 | overall loss = 0.02718\n",
      "steps = 0450 | overall loss = 0.02874\n",
      "steps = 0460 | overall loss = 0.02627\n",
      "steps = 0470 | overall loss = 0.02494\n",
      "steps = 0480 | overall loss = 0.02780\n",
      "steps = 0490 | overall loss = 0.02435\n",
      "steps = 0500 | overall loss = 0.02826\n",
      "steps = 0510 | overall loss = 0.02353\n",
      "steps = 0520 | overall loss = 0.02551\n",
      "steps = 0530 | overall loss = 0.02522\n",
      "steps = 0540 | overall loss = 0.01964\n",
      "steps = 0550 | overall loss = 0.02270\n",
      "steps = 0560 | overall loss = 0.02539\n",
      "steps = 0570 | overall loss = 0.02581\n",
      "steps = 0580 | overall loss = 0.02377\n",
      "steps = 0590 | overall loss = 0.02276\n",
      "steps = 0600 | overall loss = 0.02175\n",
      "steps = 0610 | overall loss = 0.02611\n",
      "steps = 0620 | overall loss = 0.02340\n",
      "steps = 0630 | overall loss = 0.02574\n",
      "steps = 0640 | overall loss = 0.02222\n",
      "steps = 0650 | overall loss = 0.02069\n",
      "steps = 0660 | overall loss = 0.02276\n",
      "steps = 0670 | overall loss = 0.02320\n",
      "steps = 0680 | overall loss = 0.02258\n",
      "steps = 0690 | overall loss = 0.02205\n",
      "steps = 0700 | overall loss = 0.02199\n",
      "steps = 0710 | overall loss = 0.01906\n",
      "steps = 0720 | overall loss = 0.01788\n",
      "steps = 0730 | overall loss = 0.02185\n",
      "steps = 0740 | overall loss = 0.02164\n",
      "steps = 0750 | overall loss = 0.02137\n",
      "steps = 0760 | overall loss = 0.02127\n",
      "steps = 0770 | overall loss = 0.02151\n",
      "steps = 0780 | overall loss = 0.02099\n",
      "steps = 0790 | overall loss = 0.02206\n",
      "steps = 0800 | overall loss = 0.01928\n",
      "steps = 0810 | overall loss = 0.01838\n",
      "steps = 0820 | overall loss = 0.01669\n",
      "steps = 0830 | overall loss = 0.02172\n",
      "steps = 0840 | overall loss = 0.01831\n",
      "steps = 0850 | overall loss = 0.02087\n",
      "steps = 0860 | overall loss = 0.01967\n",
      "steps = 0870 | overall loss = 0.01779\n",
      "steps = 0880 | overall loss = 0.02091\n",
      "steps = 0890 | overall loss = 0.02082\n",
      "steps = 0900 | overall loss = 0.02309\n",
      "steps = 0910 | overall loss = 0.02271\n",
      "steps = 0916 | overall loss = 0.020\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "batch_size = 6\n",
    "hidden_size = 16\n",
    "dtype = tf.float32\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "with tf.variable_scope('fooo4', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [batch_size, h, w, input_vec_size])\n",
    "    y = tf.placeholder(dtype, [batch_size, h, w, output_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "    grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    epochs = 500\n",
    "    step = 0\n",
    "    for batch in batches(data, batch_size, epochs):\n",
    "        grad_step_start_time = time()\n",
    "\n",
    "        model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "            x: np.stack([x[0] for x in batch]),\n",
    "            y: np.stack([x[1] for x in batch]),\n",
    "        })\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "\n",
    "        if tot_loss_value != tot_loss_value:\n",
    "            break\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print('steps = {0} | overall loss = {1:.3f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "    saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделът конвергира."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изследване на хиперпараметрите"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = tf.float32\n",
    "\n",
    "def train(learning_rate, batch_size, hidden_size, variable_scope):\n",
    "    with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
    "        x = tf.placeholder(dtype, [batch_size, h, w, input_vec_size])\n",
    "        y = tf.placeholder(dtype, [batch_size, h, w, output_vec_size])\n",
    "\n",
    "        mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "        rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "        model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "        grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        epochs = 10\n",
    "        step = 0\n",
    "        for batch in batches(data, batch_size, epochs):\n",
    "            grad_step_start_time = time()\n",
    "\n",
    "            model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "                x: np.stack([x[0] for x in batch]),\n",
    "                y: np.stack([x[1] for x in batch]),\n",
    "            })\n",
    "\n",
    "            print('steps = {0} | overall loss = {1:.5f} | time {2:.3f}'.format(str(step).zfill(4), tot_loss_value, time() - grad_step_start_time))\n",
    "\n",
    "            if tot_loss_value != tot_loss_value:\n",
    "                break\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нека разгледаме `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.22450 | time 7.404\n",
      "steps = 0001 | overall loss = 0.23928 | time 4.289\n",
      "steps = 0002 | overall loss = 0.19042 | time 4.279\n",
      "steps = 0003 | overall loss = 0.19908 | time 4.880\n",
      "steps = 0004 | overall loss = 0.18827 | time 4.365\n",
      "steps = 0005 | overall loss = 0.20849 | time 4.558\n",
      "steps = 0006 | overall loss = 0.19444 | time 4.335\n",
      "steps = 0007 | overall loss = 0.18227 | time 4.622\n",
      "steps = 0008 | overall loss = 0.16794 | time 5.012\n",
      "steps = 0009 | overall loss = 0.19071 | time 4.355\n",
      "steps = 0010 | overall loss = 0.18644 | time 4.441\n",
      "steps = 0011 | overall loss = 0.19184 | time 4.825\n",
      "steps = 0012 | overall loss = 0.16852 | time 4.884\n",
      "steps = 0013 | overall loss = 0.18219 | time 4.755\n",
      "steps = 0014 | overall loss = 0.18980 | time 4.537\n",
      "steps = 0015 | overall loss = 0.17211 | time 4.524\n",
      "steps = 0016 | overall loss = 0.17224 | time 4.737\n",
      "steps = 0017 | overall loss = 0.16989 | time 4.582\n",
      "steps = 0018 | overall loss = 0.17287 | time 4.516\n",
      "steps = 0019 | overall loss = 0.15418 | time 4.591\n",
      "steps = 0020 | overall loss = 0.17768 | time 4.634\n",
      "steps = 0021 | overall loss = 0.16572 | time 4.737\n",
      "steps = 0022 | overall loss = 0.16147 | time 4.758\n",
      "steps = 0023 | overall loss = 0.16892 | time 4.470\n",
      "steps = 0024 | overall loss = 0.15033 | time 4.731\n",
      "steps = 0025 | overall loss = 0.16455 | time 4.509\n",
      "steps = 0026 | overall loss = 0.16535 | time 4.583\n",
      "steps = 0027 | overall loss = 0.14471 | time 4.691\n",
      "steps = 0028 | overall loss = 0.15342 | time 4.493\n",
      "steps = 0029 | overall loss = 0.14424 | time 4.629\n",
      "steps = 0030 | overall loss = 0.16444 | time 4.633\n",
      "steps = 0031 | overall loss = 0.14894 | time 4.532\n",
      "steps = 0032 | overall loss = 0.15583 | time 4.643\n",
      "steps = 0033 | overall loss = 0.13949 | time 4.844\n",
      "steps = 0034 | overall loss = 0.14754 | time 5.020\n",
      "steps = 0035 | overall loss = 0.15638 | time 4.692\n",
      "steps = 0036 | overall loss = 0.14720 | time 4.732\n",
      "steps = 0037 | overall loss = 0.15685 | time 4.643\n",
      "steps = 0038 | overall loss = 0.15631 | time 4.706\n",
      "steps = 0039 | overall loss = 0.14372 | time 4.681\n",
      "steps = 0040 | overall loss = 0.14487 | time 4.595\n",
      "steps = 0041 | overall loss = 0.14576 | time 4.655\n",
      "steps = 0042 | overall loss = 0.14621 | time 4.620\n",
      "steps = 0043 | overall loss = 0.14158 | time 4.634\n",
      "steps = 0044 | overall loss = 0.13480 | time 4.623\n",
      "steps = 0045 | overall loss = 0.14239 | time 4.841\n",
      "steps = 0046 | overall loss = 0.14440 | time 4.679\n",
      "steps = 0047 | overall loss = 0.15449 | time 4.752\n",
      "steps = 0048 | overall loss = 0.13542 | time 4.838\n",
      "steps = 0049 | overall loss = 0.15792 | time 4.668\n",
      "steps = 0050 | overall loss = 0.14928 | time 4.723\n",
      "steps = 0051 | overall loss = 0.14227 | time 4.773\n",
      "steps = 0052 | overall loss = 0.13145 | time 4.608\n",
      "steps = 0053 | overall loss = 0.13768 | time 4.807\n",
      "steps = 0054 | overall loss = 0.13867 | time 5.341\n"
     ]
    }
   ],
   "source": [
    "train(1e-3, 2, 32, 'x1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.19480 | time 6.201\n",
      "steps = 0001 | overall loss = 0.23383 | time 3.631\n",
      "steps = 0002 | overall loss = 0.19265 | time 3.356\n",
      "steps = 0003 | overall loss = 0.20216 | time 3.388\n",
      "steps = 0004 | overall loss = 0.21528 | time 3.364\n",
      "steps = 0005 | overall loss = 0.18573 | time 3.399\n",
      "steps = 0006 | overall loss = 0.22116 | time 3.418\n",
      "steps = 0007 | overall loss = 0.19122 | time 3.432\n",
      "steps = 0008 | overall loss = 0.19894 | time 3.480\n",
      "steps = 0009 | overall loss = 0.19380 | time 3.802\n",
      "steps = 0010 | overall loss = 0.19283 | time 3.816\n",
      "steps = 0011 | overall loss = 0.17653 | time 4.121\n",
      "steps = 0012 | overall loss = 0.20932 | time 4.111\n",
      "steps = 0013 | overall loss = 0.18959 | time 4.088\n",
      "steps = 0014 | overall loss = 0.19014 | time 4.092\n",
      "steps = 0015 | overall loss = 0.19537 | time 4.136\n",
      "steps = 0016 | overall loss = 0.19537 | time 4.045\n",
      "steps = 0017 | overall loss = 0.19373 | time 4.183\n",
      "steps = 0018 | overall loss = 0.18951 | time 4.079\n",
      "steps = 0019 | overall loss = 0.18508 | time 4.004\n",
      "steps = 0020 | overall loss = 0.18357 | time 4.171\n",
      "steps = 0021 | overall loss = 0.18402 | time 4.030\n"
     ]
    }
   ],
   "source": [
    "train(1e-3, 5, 32, 'x1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = nan | time 7.857\n"
     ]
    }
   ],
   "source": [
    "train(1e-3, 8, 32, 'x1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Извод:** По-голям `batch_size` води до по-бързо обучение. Твърде голям води до числена нестабилност."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нека разгледаме `learning_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.23959 | time 8.347\n",
      "steps = 0001 | overall loss = 0.15093 | time 4.893\n",
      "steps = 0002 | overall loss = 0.16792 | time 4.884\n",
      "steps = 0003 | overall loss = 0.15135 | time 4.891\n",
      "steps = 0004 | overall loss = 0.16999 | time 5.683\n",
      "steps = 0005 | overall loss = 0.17150 | time 5.861\n",
      "steps = 0006 | overall loss = 0.17436 | time 4.892\n",
      "steps = 0007 | overall loss = 0.16512 | time 4.920\n",
      "steps = 0008 | overall loss = 0.17290 | time 4.968\n",
      "steps = 0009 | overall loss = 0.17663 | time 4.979\n",
      "steps = 0010 | overall loss = 0.16953 | time 5.051\n",
      "steps = 0011 | overall loss = 0.17352 | time 5.128\n",
      "steps = 0012 | overall loss = 0.16673 | time 5.161\n",
      "steps = 0013 | overall loss = 0.17824 | time 5.123\n",
      "steps = 0014 | overall loss = 0.17508 | time 5.201\n",
      "steps = 0015 | overall loss = 0.15553 | time 5.237\n",
      "steps = 0016 | overall loss = 0.17138 | time 5.228\n",
      "steps = 0017 | overall loss = 0.18042 | time 5.038\n",
      "steps = 0018 | overall loss = 0.18573 | time 5.076\n",
      "steps = 0019 | overall loss = 0.16574 | time 5.390\n",
      "steps = 0020 | overall loss = 0.16823 | time 5.021\n",
      "steps = 0021 | overall loss = 0.17202 | time 4.946\n"
     ]
    }
   ],
   "source": [
    "train(0.5, 5, 32, 'x1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.18755 | time 8.867\n",
      "steps = 0001 | overall loss = 0.17133 | time 5.844\n",
      "steps = 0002 | overall loss = 0.13229 | time 6.693\n",
      "steps = 0003 | overall loss = 0.12600 | time 5.389\n",
      "steps = 0004 | overall loss = 0.11812 | time 5.167\n",
      "steps = 0005 | overall loss = 0.11827 | time 4.847\n",
      "steps = 0006 | overall loss = 0.11511 | time 5.007\n",
      "steps = 0007 | overall loss = 0.10937 | time 4.914\n",
      "steps = 0008 | overall loss = 0.09374 | time 4.939\n",
      "steps = 0009 | overall loss = 0.08826 | time 4.983\n",
      "steps = 0010 | overall loss = 0.06434 | time 4.954\n",
      "steps = 0011 | overall loss = 0.06497 | time 4.984\n",
      "steps = 0012 | overall loss = 0.05683 | time 5.118\n",
      "steps = 0013 | overall loss = 0.05301 | time 4.972\n",
      "steps = 0014 | overall loss = 0.04811 | time 4.850\n",
      "steps = 0015 | overall loss = 0.05483 | time 4.984\n",
      "steps = 0016 | overall loss = 0.04312 | time 5.709\n",
      "steps = 0017 | overall loss = 0.05097 | time 5.023\n",
      "steps = 0018 | overall loss = 0.04086 | time 5.007\n",
      "steps = 0019 | overall loss = 0.03380 | time 4.974\n",
      "steps = 0020 | overall loss = 0.04399 | time 4.869\n",
      "steps = 0021 | overall loss = 0.03456 | time 5.090\n"
     ]
    }
   ],
   "source": [
    "train(0.1, 5, 32, 'x1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.24318 | time 8.537\n",
      "steps = 0001 | overall loss = 0.19397 | time 5.208\n",
      "steps = 0002 | overall loss = 0.23091 | time 5.577\n",
      "steps = 0003 | overall loss = 0.18486 | time 5.126\n",
      "steps = 0004 | overall loss = 0.18252 | time 5.016\n",
      "steps = 0005 | overall loss = 0.20282 | time 5.047\n",
      "steps = 0006 | overall loss = 0.18599 | time 5.028\n",
      "steps = 0007 | overall loss = 0.17421 | time 5.189\n",
      "steps = 0008 | overall loss = 0.18018 | time 5.024\n",
      "steps = 0009 | overall loss = 0.17664 | time 5.052\n",
      "steps = 0010 | overall loss = 0.16181 | time 5.108\n",
      "steps = 0011 | overall loss = 0.17543 | time 5.039\n",
      "steps = 0012 | overall loss = 0.15204 | time 5.059\n",
      "steps = 0013 | overall loss = 0.16106 | time 5.031\n",
      "steps = 0014 | overall loss = 0.15105 | time 5.068\n",
      "steps = 0015 | overall loss = 0.16661 | time 5.045\n",
      "steps = 0016 | overall loss = 0.14461 | time 5.023\n",
      "steps = 0017 | overall loss = 0.15033 | time 6.557\n",
      "steps = 0018 | overall loss = 0.14266 | time 5.014\n",
      "steps = 0019 | overall loss = 0.14872 | time 5.097\n",
      "steps = 0020 | overall loss = 0.15395 | time 5.067\n",
      "steps = 0021 | overall loss = 0.12903 | time 5.127\n"
     ]
    }
   ],
   "source": [
    "train(0.01, 5, 32, 'x1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Извод:** Изненадващо. При по-ранни експерименти всяка малко по-висока стойност на learning rate водеше до числена нестабилност. Възможно е да е било заради по-големият размер на картите (128x128 тогава, 64x64 сега). За сега ще оставим този въпрос отворен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нека разгледаме `hidden_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.26832 | time 7.288\n",
      "steps = 0001 | overall loss = 0.19507 | time 3.657\n",
      "steps = 0002 | overall loss = 0.21288 | time 3.855\n",
      "steps = 0003 | overall loss = 0.19213 | time 3.627\n",
      "steps = 0004 | overall loss = 0.18615 | time 3.628\n",
      "steps = 0005 | overall loss = 0.18491 | time 3.608\n",
      "steps = 0006 | overall loss = 0.18188 | time 3.668\n",
      "steps = 0007 | overall loss = 0.17009 | time 3.612\n",
      "steps = 0008 | overall loss = 0.18564 | time 3.593\n",
      "steps = 0009 | overall loss = 0.16213 | time 3.603\n",
      "steps = 0010 | overall loss = 0.17815 | time 3.651\n",
      "steps = 0011 | overall loss = 0.16675 | time 3.891\n",
      "steps = 0012 | overall loss = 0.17001 | time 3.712\n",
      "steps = 0013 | overall loss = 0.18198 | time 3.710\n",
      "steps = 0014 | overall loss = 0.15322 | time 3.710\n",
      "steps = 0015 | overall loss = 0.15681 | time 3.923\n",
      "steps = 0016 | overall loss = 0.17712 | time 4.003\n",
      "steps = 0017 | overall loss = 0.16453 | time 4.026\n",
      "steps = 0018 | overall loss = 0.15952 | time 4.041\n",
      "steps = 0019 | overall loss = 0.15602 | time 4.151\n",
      "steps = 0020 | overall loss = 0.15990 | time 4.325\n",
      "steps = 0021 | overall loss = 0.14829 | time 4.423\n"
     ]
    }
   ],
   "source": [
    "train(0.01, 5, 8, 'x5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.24190 | time 8.845\n",
      "steps = 0001 | overall loss = 0.19243 | time 5.362\n",
      "steps = 0002 | overall loss = 0.19146 | time 5.426\n",
      "steps = 0003 | overall loss = 0.19279 | time 5.388\n",
      "steps = 0004 | overall loss = 0.18755 | time 5.400\n",
      "steps = 0005 | overall loss = 0.17310 | time 5.512\n",
      "steps = 0006 | overall loss = 0.17268 | time 5.598\n",
      "steps = 0007 | overall loss = 0.16584 | time 5.426\n",
      "steps = 0008 | overall loss = 0.16395 | time 5.387\n",
      "steps = 0009 | overall loss = 0.16531 | time 5.583\n",
      "steps = 0010 | overall loss = 0.15367 | time 7.052\n",
      "steps = 0011 | overall loss = 0.15557 | time 5.301\n",
      "steps = 0012 | overall loss = 0.14952 | time 6.330\n",
      "steps = 0013 | overall loss = 0.16014 | time 5.378\n",
      "steps = 0014 | overall loss = 0.14689 | time 5.415\n",
      "steps = 0015 | overall loss = 0.14865 | time 5.512\n",
      "steps = 0016 | overall loss = 0.14717 | time 5.303\n",
      "steps = 0017 | overall loss = 0.14969 | time 5.448\n",
      "steps = 0018 | overall loss = 0.14041 | time 5.460\n",
      "steps = 0019 | overall loss = 0.14192 | time 5.828\n",
      "steps = 0020 | overall loss = 0.14870 | time 5.509\n",
      "steps = 0021 | overall loss = 0.13877 | time 5.430\n"
     ]
    }
   ],
   "source": [
    "train(0.01, 5, 32, 'x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.23936 | time 28.969\n",
      "steps = 0001 | overall loss = 0.18906 | time 25.882\n",
      "steps = 0002 | overall loss = 0.18992 | time 26.147\n",
      "steps = 0003 | overall loss = 0.18570 | time 26.899\n",
      "steps = 0004 | overall loss = 0.18603 | time 27.369\n",
      "steps = 0005 | overall loss = 0.16311 | time 28.521\n",
      "steps = 0006 | overall loss = 0.15637 | time 26.775\n",
      "steps = 0007 | overall loss = 0.15852 | time 26.869\n",
      "steps = 0008 | overall loss = 0.14762 | time 27.390\n",
      "steps = 0009 | overall loss = 0.14391 | time 26.984\n",
      "steps = 0010 | overall loss = 0.14746 | time 26.791\n",
      "steps = 0011 | overall loss = 0.14289 | time 26.477\n",
      "steps = 0012 | overall loss = 0.14387 | time 25.718\n",
      "steps = 0013 | overall loss = 0.12873 | time 25.919\n",
      "steps = 0014 | overall loss = 0.13779 | time 28.071\n",
      "steps = 0015 | overall loss = 0.14269 | time 28.430\n",
      "steps = 0016 | overall loss = 0.13338 | time 25.671\n",
      "steps = 0017 | overall loss = 0.13118 | time 29.361\n",
      "steps = 0018 | overall loss = 0.13405 | time 26.839\n",
      "steps = 0019 | overall loss = 0.12724 | time 26.530\n",
      "steps = 0020 | overall loss = 0.11983 | time 28.054\n",
      "steps = 0021 | overall loss = 0.12796 | time 26.105\n"
     ]
    }
   ],
   "source": [
    "train(0.01, 5, 128, 'x4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Извод:** Ефект има. Но е почти пренебрежим. За сметка на доста по-бавно време за трениране. С по-дълги тренирания може да видим по-смислена разлика. Нека пробваме по-дълго трениране..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.24157\n",
      "steps = 0010 | overall loss = 0.18574\n",
      "steps = 0020 | overall loss = 0.17465\n",
      "steps = 0030 | overall loss = 0.16504\n",
      "steps = 0040 | overall loss = 0.15868\n",
      "steps = 0050 | overall loss = 0.15054\n",
      "steps = 0060 | overall loss = 0.14977\n",
      "steps = 0070 | overall loss = 0.13328\n",
      "steps = 0080 | overall loss = 0.13253\n",
      "steps = 0090 | overall loss = 0.12998\n",
      "steps = 0100 | overall loss = 0.12211\n",
      "steps = 0110 | overall loss = 0.11872\n",
      "steps = 0120 | overall loss = 0.11888\n",
      "steps = 0130 | overall loss = 0.11038\n",
      "steps = 0140 | overall loss = 0.10638\n",
      "steps = 0150 | overall loss = 0.09887\n",
      "steps = 0160 | overall loss = 0.10004\n",
      "steps = 0170 | overall loss = 0.09228\n",
      "steps = 0180 | overall loss = 0.09305\n",
      "steps = 0190 | overall loss = 0.08365\n",
      "steps = 0200 | overall loss = 0.08484\n",
      "steps = 0210 | overall loss = 0.07976\n",
      "steps = 0220 | overall loss = 0.07927\n",
      "steps = 0230 | overall loss = 0.07513\n",
      "steps = 0240 | overall loss = 0.07296\n",
      "steps = 0250 | overall loss = 0.06869\n",
      "steps = 0260 | overall loss = 0.06693\n",
      "steps = 0270 | overall loss = 0.06637\n",
      "steps = 0280 | overall loss = 0.06620\n",
      "steps = 0290 | overall loss = 0.06114\n",
      "steps = 0300 | overall loss = 0.06247\n",
      "steps = 0310 | overall loss = 0.05903\n",
      "steps = 0320 | overall loss = 0.06056\n",
      "steps = 0330 | overall loss = 0.05701\n",
      "steps = 0340 | overall loss = 0.05646\n",
      "steps = 0350 | overall loss = 0.05634\n",
      "steps = 0360 | overall loss = 0.05480\n",
      "steps = 0370 | overall loss = 0.05430\n",
      "steps = 0380 | overall loss = 0.05113\n",
      "steps = 0390 | overall loss = 0.05114\n",
      "steps = 0400 | overall loss = 0.04569\n",
      "steps = 0410 | overall loss = 0.04823\n",
      "steps = 0420 | overall loss = 0.04778\n",
      "steps = 0430 | overall loss = 0.04454\n",
      "steps = 0440 | overall loss = 0.04515\n",
      "steps = 0450 | overall loss = 0.04354\n",
      "steps = 0460 | overall loss = 0.04522\n",
      "steps = 0470 | overall loss = 0.04327\n",
      "steps = 0480 | overall loss = 0.04177\n",
      "steps = 0490 | overall loss = 0.04246\n",
      "steps = 0500 | overall loss = 0.04064\n",
      "steps = 0510 | overall loss = 0.04006\n",
      "steps = 0520 | overall loss = 0.04045\n",
      "steps = 0530 | overall loss = 0.04170\n",
      "steps = 0540 | overall loss = 0.03435\n",
      "steps = 0550 | overall loss = 0.03882\n",
      "steps = 0560 | overall loss = 0.03697\n",
      "steps = 0570 | overall loss = 0.03358\n",
      "steps = 0580 | overall loss = 0.03600\n",
      "steps = 0590 | overall loss = 0.03612\n",
      "steps = 0600 | overall loss = 0.03083\n",
      "steps = 0610 | overall loss = 0.03185\n",
      "steps = 0620 | overall loss = 0.03242\n",
      "steps = 0630 | overall loss = 0.03280\n",
      "steps = 0640 | overall loss = 0.03213\n",
      "steps = 0650 | overall loss = 0.02987\n",
      "steps = 0660 | overall loss = 0.02767\n",
      "steps = 0670 | overall loss = 0.02927\n",
      "steps = 0680 | overall loss = 0.02697\n",
      "steps = 0690 | overall loss = 0.02667\n",
      "steps = 0700 | overall loss = 0.03024\n",
      "steps = 0710 | overall loss = 0.02541\n",
      "steps = 0720 | overall loss = 0.02851\n",
      "steps = 0730 | overall loss = 0.02528\n",
      "steps = 0740 | overall loss = 0.02565\n",
      "steps = 0750 | overall loss = 0.02448\n",
      "steps = 0760 | overall loss = 0.02642\n",
      "steps = 0770 | overall loss = 0.02594\n",
      "steps = 0780 | overall loss = 0.02448\n",
      "steps = 0785 | overall loss = 0.02642\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-3\n",
    "batch_size = 7\n",
    "hidden_size = 32\n",
    "dtype = tf.float32\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "with tf.variable_scope('fooo5', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [batch_size, h, w, input_vec_size])\n",
    "    y = tf.placeholder(dtype, [batch_size, h, w, output_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "    grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    epochs = 500\n",
    "    step = 0\n",
    "    for batch in batches(data, batch_size, epochs):\n",
    "        grad_step_start_time = time()\n",
    "\n",
    "        model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "            x: np.stack([x[0] for x in batch]),\n",
    "            y: np.stack([x[1] for x in batch]),\n",
    "        })\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "\n",
    "        if tot_loss_value != tot_loss_value:\n",
    "            break\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "    saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.21187\n",
      "steps = 0010 | overall loss = 0.17123\n",
      "steps = 0020 | overall loss = 0.15428\n",
      "steps = 0030 | overall loss = 0.14543\n",
      "steps = 0040 | overall loss = 0.13634\n",
      "steps = 0050 | overall loss = 0.12234\n",
      "steps = 0060 | overall loss = 0.10602\n",
      "steps = 0070 | overall loss = 0.10388\n",
      "steps = 0080 | overall loss = 0.09660\n",
      "steps = 0090 | overall loss = 0.08475\n",
      "steps = 0100 | overall loss = 0.07905\n",
      "steps = 0110 | overall loss = 0.07268\n",
      "steps = 0120 | overall loss = 0.06687\n",
      "steps = 0130 | overall loss = 0.06302\n",
      "steps = 0140 | overall loss = 0.05441\n",
      "steps = 0150 | overall loss = 0.05287\n",
      "steps = 0160 | overall loss = 0.05104\n",
      "steps = 0170 | overall loss = 0.04304\n",
      "steps = 0180 | overall loss = 0.04698\n",
      "steps = 0190 | overall loss = 0.04179\n",
      "steps = 0200 | overall loss = 0.03907\n",
      "steps = 0210 | overall loss = 0.03793\n",
      "steps = 0220 | overall loss = 0.03540\n",
      "steps = 0230 | overall loss = 0.03433\n",
      "steps = 0240 | overall loss = 0.03191\n",
      "steps = 0250 | overall loss = 0.03108\n",
      "steps = 0260 | overall loss = 0.02836\n",
      "steps = 0270 | overall loss = 0.02772\n",
      "steps = 0280 | overall loss = 0.02748\n",
      "steps = 0290 | overall loss = 0.02821\n",
      "steps = 0300 | overall loss = 0.02603\n",
      "steps = 0310 | overall loss = 0.02458\n",
      "steps = 0320 | overall loss = 0.02581\n",
      "steps = 0330 | overall loss = 0.02475\n",
      "steps = 0340 | overall loss = 0.02362\n",
      "steps = 0350 | overall loss = 0.02448\n",
      "steps = 0360 | overall loss = 0.02307\n",
      "steps = 0370 | overall loss = 0.02217\n",
      "steps = 0380 | overall loss = 0.02278\n",
      "steps = 0390 | overall loss = 0.02383\n",
      "steps = 0400 | overall loss = 0.02442\n",
      "steps = 0410 | overall loss = 0.02132\n",
      "steps = 0420 | overall loss = 0.02180\n",
      "steps = 0430 | overall loss = 0.02053\n",
      "steps = 0440 | overall loss = 0.01962\n",
      "steps = 0450 | overall loss = 0.02078\n",
      "steps = 0460 | overall loss = 0.02054\n",
      "steps = 0470 | overall loss = 0.02108\n",
      "steps = 0480 | overall loss = 0.02141\n",
      "steps = 0490 | overall loss = 0.02008\n",
      "steps = 0500 | overall loss = 0.02018\n",
      "steps = 0510 | overall loss = 0.01798\n",
      "steps = 0520 | overall loss = 0.01640\n",
      "steps = 0530 | overall loss = 0.01988\n",
      "steps = 0540 | overall loss = 0.02145\n",
      "steps = 0550 | overall loss = 0.01870\n",
      "steps = 0560 | overall loss = 0.01968\n",
      "steps = 0570 | overall loss = 0.01646\n",
      "steps = 0580 | overall loss = 0.01932\n",
      "steps = 0590 | overall loss = 0.01712\n",
      "steps = 0600 | overall loss = 0.01546\n",
      "steps = 0610 | overall loss = 0.01739\n",
      "steps = 0620 | overall loss = 0.01708\n",
      "steps = 0630 | overall loss = 0.01952\n",
      "steps = 0640 | overall loss = 0.01706\n",
      "steps = 0650 | overall loss = 0.01355\n",
      "steps = 0660 | overall loss = 0.01497\n",
      "steps = 0670 | overall loss = 0.01731\n",
      "steps = 0680 | overall loss = 0.01725\n",
      "steps = 0690 | overall loss = 0.01632\n",
      "steps = 0700 | overall loss = 0.01589\n",
      "steps = 0710 | overall loss = 0.01888\n",
      "steps = 0720 | overall loss = 0.01731\n",
      "steps = 0730 | overall loss = 0.01420\n",
      "steps = 0740 | overall loss = 0.01503\n",
      "steps = 0750 | overall loss = 0.01609\n",
      "steps = 0760 | overall loss = 0.01351\n",
      "steps = 0770 | overall loss = 0.01600\n",
      "steps = 0780 | overall loss = 0.01593\n",
      "steps = 0790 | overall loss = 0.01427\n",
      "steps = 0800 | overall loss = 0.01560\n",
      "steps = 0810 | overall loss = 0.01530\n",
      "steps = 0820 | overall loss = 0.01285\n",
      "steps = 0830 | overall loss = 0.01263\n",
      "steps = 0840 | overall loss = 0.01535\n",
      "steps = 0850 | overall loss = 0.01456\n",
      "steps = 0860 | overall loss = 0.01430\n",
      "steps = 0870 | overall loss = 0.01388\n",
      "steps = 0880 | overall loss = 0.01202\n",
      "steps = 0890 | overall loss = 0.01418\n",
      "steps = 0900 | overall loss = 0.01672\n",
      "steps = 0910 | overall loss = 0.01600\n",
      "steps = 0920 | overall loss = 0.01654\n",
      "steps = 0930 | overall loss = 0.01453\n",
      "steps = 0940 | overall loss = 0.01239\n",
      "steps = 0950 | overall loss = 0.01334\n",
      "steps = 0960 | overall loss = 0.01353\n",
      "steps = 0970 | overall loss = 0.01376\n",
      "steps = 0980 | overall loss = 0.01197\n",
      "steps = 0990 | overall loss = 0.01294\n",
      "steps = 1000 | overall loss = 0.01207\n",
      "steps = 1010 | overall loss = 0.01391\n",
      "steps = 1020 | overall loss = 0.01263\n",
      "steps = 1030 | overall loss = 0.01036\n",
      "steps = 1040 | overall loss = 0.01274\n",
      "steps = 1050 | overall loss = 0.01342\n",
      "steps = 1060 | overall loss = 0.01087\n",
      "steps = 1070 | overall loss = 0.01291\n",
      "steps = 1080 | overall loss = 0.01176\n",
      "steps = 1090 | overall loss = 0.01160\n",
      "steps = 1100 | overall loss = 0.01175\n",
      "steps = 1110 | overall loss = 0.01227\n",
      "steps = 1120 | overall loss = 0.01233\n",
      "steps = 1130 | overall loss = 0.01150\n",
      "steps = 1140 | overall loss = 0.01138\n",
      "steps = 1150 | overall loss = 0.01113\n",
      "steps = 1160 | overall loss = 0.01137\n",
      "steps = 1170 | overall loss = 0.01117\n",
      "steps = 1180 | overall loss = 0.01007\n",
      "steps = 1190 | overall loss = 0.01202\n",
      "steps = 1200 | overall loss = 0.01022\n",
      "steps = 1210 | overall loss = 0.01120\n",
      "steps = 1220 | overall loss = 0.01145\n",
      "steps = 1230 | overall loss = 0.01325\n",
      "steps = 1240 | overall loss = 0.01265\n",
      "steps = 1250 | overall loss = 0.01032\n",
      "steps = 1260 | overall loss = 0.01117\n",
      "steps = 1270 | overall loss = 0.00990\n",
      "steps = 1280 | overall loss = 0.00929\n",
      "steps = 1290 | overall loss = 0.01012\n",
      "steps = 1300 | overall loss = 0.01002\n",
      "steps = 1310 | overall loss = 0.01158\n",
      "steps = 1320 | overall loss = 0.01134\n",
      "steps = 1330 | overall loss = 0.01002\n",
      "steps = 1340 | overall loss = 0.00972\n",
      "steps = 1350 | overall loss = 0.00936\n",
      "steps = 1360 | overall loss = 0.01035\n",
      "steps = 1370 | overall loss = 0.00875\n",
      "steps = 1380 | overall loss = 0.01110\n",
      "steps = 1390 | overall loss = 0.01157\n",
      "steps = 1400 | overall loss = 0.01005\n",
      "steps = 1410 | overall loss = 0.01030\n",
      "steps = 1420 | overall loss = 0.01017\n",
      "steps = 1430 | overall loss = 0.01076\n",
      "steps = 1440 | overall loss = 0.01120\n",
      "steps = 1450 | overall loss = 0.00839\n",
      "steps = 1460 | overall loss = 0.00892\n",
      "steps = 1470 | overall loss = 0.00810\n",
      "steps = 1480 | overall loss = 0.00835\n",
      "steps = 1490 | overall loss = 0.00978\n",
      "steps = 1500 | overall loss = 0.00968\n",
      "steps = 1510 | overall loss = 0.00847\n",
      "steps = 1520 | overall loss = 0.00879\n",
      "steps = 1530 | overall loss = 0.00770\n",
      "steps = 1540 | overall loss = 0.00936\n",
      "steps = 1550 | overall loss = 0.00889\n",
      "steps = 1560 | overall loss = 0.00726\n",
      "steps = 1570 | overall loss = 0.00865\n",
      "steps = 1580 | overall loss = 0.00902\n",
      "steps = 1590 | overall loss = 0.00852\n",
      "steps = 1600 | overall loss = 0.00811\n",
      "steps = 1610 | overall loss = 0.00741\n",
      "steps = 1620 | overall loss = 0.00889\n",
      "steps = 1630 | overall loss = 0.00966\n",
      "steps = 1640 | overall loss = 0.00977\n",
      "steps = 1650 | overall loss = 0.00856\n",
      "steps = 1660 | overall loss = 0.01012\n",
      "steps = 1670 | overall loss = 0.00871\n",
      "steps = 1680 | overall loss = 0.00915\n",
      "steps = 1690 | overall loss = 0.00774\n",
      "steps = 1700 | overall loss = 0.00967\n",
      "steps = 1710 | overall loss = 0.00884\n",
      "steps = 1720 | overall loss = 0.00948\n",
      "steps = 1730 | overall loss = 0.00762\n",
      "steps = 1740 | overall loss = 0.00834\n",
      "steps = 1750 | overall loss = 0.00954\n",
      "steps = 1760 | overall loss = 0.00866\n",
      "steps = 1770 | overall loss = 0.00709\n",
      "steps = 1780 | overall loss = 0.00755\n",
      "steps = 1790 | overall loss = 0.00788\n",
      "steps = 1800 | overall loss = 0.00845\n",
      "steps = 1810 | overall loss = 0.00891\n",
      "steps = 1820 | overall loss = 0.00791\n",
      "steps = 1830 | overall loss = 0.00830\n",
      "steps = 1840 | overall loss = 0.00762\n",
      "steps = 1850 | overall loss = 0.00873\n",
      "steps = 1860 | overall loss = 0.00806\n",
      "steps = 1870 | overall loss = 0.00766\n",
      "steps = 1880 | overall loss = 0.00780\n",
      "steps = 1890 | overall loss = 0.00697\n",
      "steps = 1900 | overall loss = 0.00680\n",
      "steps = 1910 | overall loss = 0.00894\n",
      "steps = 1920 | overall loss = 0.00820\n",
      "steps = 1930 | overall loss = 0.00687\n",
      "steps = 1940 | overall loss = 0.00813\n",
      "steps = 1950 | overall loss = 0.00908\n",
      "steps = 1960 | overall loss = 0.00799\n",
      "steps = 1970 | overall loss = 0.00740\n",
      "steps = 1980 | overall loss = 0.00782\n",
      "steps = 1990 | overall loss = 0.00676\n",
      "steps = 2000 | overall loss = 0.00737\n",
      "steps = 2010 | overall loss = 0.00718\n",
      "steps = 2020 | overall loss = 0.00690\n",
      "steps = 2030 | overall loss = 0.00781\n",
      "steps = 2040 | overall loss = 0.00909\n",
      "steps = 2050 | overall loss = 0.00878\n",
      "steps = 2060 | overall loss = 0.00717\n",
      "steps = 2070 | overall loss = 0.00633\n",
      "steps = 2080 | overall loss = 0.00646\n",
      "steps = 2090 | overall loss = 0.00637\n",
      "steps = 2100 | overall loss = 0.00774\n",
      "steps = 2110 | overall loss = 0.00809\n",
      "steps = 2120 | overall loss = 0.00705\n",
      "steps = 2130 | overall loss = 0.00694\n",
      "steps = 2140 | overall loss = 0.00756\n",
      "steps = 2150 | overall loss = 0.00716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 2160 | overall loss = 0.00679\n",
      "steps = 2170 | overall loss = 0.00659\n",
      "steps = 2180 | overall loss = 0.00711\n",
      "steps = 2190 | overall loss = 0.00589\n",
      "steps = 2200 | overall loss = 0.00651\n",
      "steps = 2210 | overall loss = 0.00888\n",
      "steps = 2220 | overall loss = 0.00951\n",
      "steps = 2230 | overall loss = 0.00762\n",
      "steps = 2240 | overall loss = 0.00665\n",
      "steps = 2250 | overall loss = 0.00823\n",
      "steps = 2260 | overall loss = 0.00601\n",
      "steps = 2270 | overall loss = 0.00690\n",
      "steps = 2280 | overall loss = 0.00599\n",
      "steps = 2290 | overall loss = 0.00608\n",
      "steps = 2300 | overall loss = 0.00624\n",
      "steps = 2310 | overall loss = 0.00658\n",
      "steps = 2320 | overall loss = 0.00798\n",
      "steps = 2330 | overall loss = 0.00728\n",
      "steps = 2340 | overall loss = 0.00623\n",
      "steps = 2350 | overall loss = 0.00629\n",
      "steps = 2360 | overall loss = 0.00529\n",
      "steps = 2370 | overall loss = 0.00715\n",
      "steps = 2380 | overall loss = 0.00633\n",
      "steps = 2390 | overall loss = 0.00761\n",
      "steps = 2400 | overall loss = 0.00662\n",
      "steps = 2410 | overall loss = 0.00717\n",
      "steps = 2420 | overall loss = 0.00761\n",
      "steps = 2430 | overall loss = 0.00763\n",
      "steps = 2440 | overall loss = 0.00682\n",
      "steps = 2450 | overall loss = 0.00707\n",
      "steps = 2460 | overall loss = 0.00696\n",
      "steps = 2470 | overall loss = 0.00542\n",
      "steps = 2480 | overall loss = 0.00618\n",
      "steps = 2490 | overall loss = 0.00649\n",
      "steps = 2500 | overall loss = 0.00583\n",
      "steps = 2510 | overall loss = 0.00694\n",
      "steps = 2520 | overall loss = 0.00552\n",
      "steps = 2530 | overall loss = 0.00600\n",
      "steps = 2540 | overall loss = 0.00597\n",
      "steps = 2550 | overall loss = 0.00640\n",
      "steps = 2560 | overall loss = 0.00505\n",
      "steps = 2570 | overall loss = 0.00542\n",
      "steps = 2580 | overall loss = 0.00526\n",
      "steps = 2590 | overall loss = 0.00867\n",
      "steps = 2600 | overall loss = 0.00802\n",
      "steps = 2610 | overall loss = 0.00721\n",
      "steps = 2620 | overall loss = 0.00622\n",
      "steps = 2630 | overall loss = 0.00605\n",
      "steps = 2640 | overall loss = 0.00575\n",
      "steps = 2650 | overall loss = 0.00593\n",
      "steps = 2660 | overall loss = 0.00532\n",
      "steps = 2670 | overall loss = 0.00608\n",
      "steps = 2680 | overall loss = 0.00524\n",
      "steps = 2690 | overall loss = 0.00445\n",
      "steps = 2700 | overall loss = 0.00576\n",
      "steps = 2710 | overall loss = 0.00717\n",
      "steps = 2720 | overall loss = 0.00609\n",
      "steps = 2730 | overall loss = 0.00528\n",
      "steps = 2740 | overall loss = 0.00557\n",
      "steps = 2750 | overall loss = 0.00517\n",
      "steps = 2760 | overall loss = 0.00473\n",
      "steps = 2770 | overall loss = 0.00697\n",
      "steps = 2780 | overall loss = 0.00680\n",
      "steps = 2790 | overall loss = 0.00599\n",
      "steps = 2800 | overall loss = 0.00694\n",
      "steps = 2810 | overall loss = 0.00704\n",
      "steps = 2820 | overall loss = 0.00619\n",
      "steps = 2830 | overall loss = 0.00630\n",
      "steps = 2840 | overall loss = 0.00593\n",
      "steps = 2850 | overall loss = 0.00492\n",
      "steps = 2860 | overall loss = 0.00465\n",
      "steps = 2870 | overall loss = 0.00507\n",
      "steps = 2880 | overall loss = 0.00464\n",
      "steps = 2890 | overall loss = 0.00474\n",
      "steps = 2900 | overall loss = 0.00489\n",
      "steps = 2910 | overall loss = 0.00527\n",
      "steps = 2920 | overall loss = 0.00540\n",
      "steps = 2930 | overall loss = 0.00579\n",
      "steps = 2940 | overall loss = 0.00477\n",
      "steps = 2950 | overall loss = 0.00510\n",
      "steps = 2960 | overall loss = 0.00626\n",
      "steps = 2970 | overall loss = 0.00586\n",
      "steps = 2980 | overall loss = 0.00643\n",
      "steps = 2990 | overall loss = 0.00605\n",
      "steps = 3000 | overall loss = 0.00569\n",
      "steps = 3010 | overall loss = 0.00490\n",
      "steps = 3020 | overall loss = 0.00542\n",
      "steps = 3030 | overall loss = 0.00446\n",
      "steps = 3040 | overall loss = 0.00456\n",
      "steps = 3050 | overall loss = 0.00479\n",
      "steps = 3060 | overall loss = 0.00654\n",
      "steps = 3070 | overall loss = 0.00598\n",
      "steps = 3080 | overall loss = 0.00584\n",
      "steps = 3090 | overall loss = 0.00742\n",
      "steps = 3100 | overall loss = 0.00679\n",
      "steps = 3110 | overall loss = 0.00640\n",
      "steps = 3120 | overall loss = 0.00528\n",
      "steps = 3130 | overall loss = 0.00498\n",
      "steps = 3140 | overall loss = 0.00437\n",
      "steps = 3142 | overall loss = 0.00552\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-3\n",
    "batch_size = 7\n",
    "hidden_size = 64\n",
    "dtype = tf.float32\n",
    "\n",
    "with tf.variable_scope('fooo6', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [None, h, w, input_vec_size])\n",
    "    y = tf.placeholder(dtype, [None, h, w, output_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "    grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    epochs = 2000\n",
    "    step = 0\n",
    "    for batch in batches(data, batch_size, epochs):\n",
    "        model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "            x: np.stack([x[0] for x in batch]),\n",
    "            y: np.stack([x[1] for x in batch]),\n",
    "        })\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "\n",
    "        if tot_loss_value != tot_loss_value:\n",
    "            break\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "    saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На стъпка 780 виждаме, че от по-голям размер на hidden state има доста смисъл. Оставихме второто трениране да мине през повече епохи след стъпка 780 - но това е с друга цел.\n",
    "\n",
    "**Извод:** След достатъчно голям брой епохи започва да се вижда смисъл от `hidden_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семпилиране на модела\n",
    "\n",
    "Ще напишем малко дървен (и ужасно бавен) код, с който да ползваме модела, за да генерираме карти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_by_layers(height, width):\n",
    "    \n",
    "    def iterate_layer(layer):\n",
    "        for x in range(layer + 1):\n",
    "            y = layer - x\n",
    "            if 0 <= x < width and 0 <= y < height:\n",
    "                yield (x, y)\n",
    "\n",
    "    for layer in range(width + height - 1):\n",
    "        yield iterate_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cosine_similarity = lambda a, b: dot(a, b) / (norm(a) * norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33343333, 1.0001    , 1.0001    ], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_func = lambda x: cosine_similarity(x, model_preds[0, 0, 0, :])\n",
    "sorted(list(map(to_features, Tileset.JUNGLE.tiles)), reverse=True, key=key_func)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.variable_scope('fooo6', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [1, h, w, input_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    inputs = np.zeros([1, 64, 64, 6])\n",
    "    ouputs = np.zeros([1, 64, 64, 3])\n",
    "    \n",
    "    for layer in iterate_by_layers(64, 64):\n",
    "        l = list(layer)\n",
    "        for i, j in l:\n",
    "            inputs[0, i, j, 0:3] = ouputs[0, i - 1, j, :]\n",
    "            inputs[0, i, j, 3:6] = ouputs[0, i, j - 1, :]\n",
    "            \n",
    "        model_preds = sess.run(model_out, feed_dict={ x: inputs })\n",
    "\n",
    "        for i, j in l:\n",
    "            key_func = lambda x: cosine_similarity(x, model_preds[0, i, j, :])\n",
    "            most_similar_tile_features = sorted(list(map(to_features, Tileset.JUNGLE.tiles)), reverse=True, key=key_func)[0]\n",
    "            ouputs[0, i, j] = most_similar_tile_features\n",
    "            \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.imshow(model_preds[0, :, :, i], cmap=plt.cm.Blues)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Това, което виждаме горе са предложенията на модела за всеки от feature-ите на плочките. Те са записани в променливата `model_preds`.\n",
    "\n",
    "На всяка стъпка взимаме плчоката с feature-и най-близки до предложените и вкарваме feature-ите на реалната плочка обратно в модела. Feature-ите на реалните плочки за записани в `outputs`. Нека видим как изглеждат те..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(ouputs[0, :, :, i], cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Извод:** семплирането е пълен провал."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Да опитаме да тренираме отново\n",
    "\n",
    "Този път с по-висок learning rate и по-голям размер на скрития слой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.23238 | time 48.053\n",
      "steps = 0001 | overall loss = 0.17653 | time 50.446\n",
      "steps = 0002 | overall loss = 0.13012 | time 43.616\n",
      "steps = 0003 | overall loss = 0.12293 | time 45.608\n",
      "steps = 0004 | overall loss = 0.13487 | time 45.471\n",
      "steps = 0005 | overall loss = 0.13824 | time 47.846\n",
      "steps = 0006 | overall loss = 0.11992 | time 46.841\n",
      "steps = 0007 | overall loss = 0.12499 | time 44.547\n",
      "steps = 0008 | overall loss = 0.11788 | time 49.146\n",
      "steps = 0009 | overall loss = 0.10472 | time 44.285\n",
      "steps = 0010 | overall loss = 0.10316 | time 49.340\n",
      "steps = 0011 | overall loss = 0.10070 | time 49.343\n",
      "steps = 0012 | overall loss = 0.08847 | time 47.114\n",
      "steps = 0013 | overall loss = 0.07617 | time 47.578\n",
      "steps = 0014 | overall loss = 0.07680 | time 47.851\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "batch_size = 7\n",
    "hidden_size = 128\n",
    "dtype = tf.float32\n",
    "\n",
    "with tf.variable_scope('fooo7', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [None, h, w, input_vec_size])\n",
    "    y = tf.placeholder(dtype, [None, h, w, output_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "    grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    epochs = 10\n",
    "    step = 0\n",
    "    for batch in batches(data, batch_size, epochs):\n",
    "        grad_step_start_time = time()\n",
    "\n",
    "        model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "            x: np.stack([x[0] for x in batch]),\n",
    "            y: np.stack([x[1] for x in batch]),\n",
    "        })\n",
    "\n",
    "        print('steps = {0} | overall loss = {1:.5f} | time {2:.3f}'.format(\n",
    "            str(step).zfill(4),\n",
    "            tot_loss_value,\n",
    "            time() - grad_step_start_time))\n",
    "\n",
    "        if tot_loss_value != tot_loss_value:\n",
    "            break\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.22102 | time 54.358\n",
      "steps = 0001 | overall loss = 0.19691 | time 49.380\n",
      "steps = 0002 | overall loss = 0.14738 | time 50.632\n",
      "steps = 0003 | overall loss = 0.13613 | time 47.807\n",
      "steps = 0004 | overall loss = 0.12611 | time 46.935\n",
      "steps = 0005 | overall loss = 0.11668 | time 49.374\n",
      "steps = 0006 | overall loss = 0.11018 | time 50.137\n",
      "steps = 0007 | overall loss = 0.10022 | time 54.867\n",
      "steps = 0008 | overall loss = 0.09497 | time 48.338\n",
      "steps = 0009 | overall loss = 0.08378 | time 53.585\n",
      "steps = 0010 | overall loss = 0.07928 | time 52.399\n",
      "steps = 0011 | overall loss = 0.07206 | time 48.528\n",
      "steps = 0012 | overall loss = 0.05803 | time 48.583\n",
      "steps = 0013 | overall loss = 0.05445 | time 51.136\n",
      "steps = 0014 | overall loss = 0.05386 | time 49.033\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "batch_size = 7\n",
    "hidden_size = 128\n",
    "dtype = tf.float32\n",
    "\n",
    "with tf.variable_scope('fooo7', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [None, h, w, input_vec_size])\n",
    "    y = tf.placeholder(dtype, [None, h, w, output_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "    grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    epochs = 10\n",
    "    step = 0\n",
    "    for batch in batches(data, batch_size, epochs):\n",
    "        grad_step_start_time = time()\n",
    "\n",
    "        model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "            x: np.stack([x[0] for x in batch]),\n",
    "            y: np.stack([x[1] for x in batch]),\n",
    "        })\n",
    "\n",
    "        print('steps = {0} | overall loss = {1:.5f} | time {2:.3f}'.format(\n",
    "            str(step).zfill(4),\n",
    "            tot_loss_value,\n",
    "            time() - grad_step_start_time))\n",
    "\n",
    "        if tot_loss_value != tot_loss_value:\n",
    "            break\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.21940 | time 63.271\n",
      "steps = 0001 | overall loss = 0.18806 | time 56.306\n",
      "steps = 0002 | overall loss = 0.18016 | time 58.864\n",
      "steps = 0003 | overall loss = 0.15036 | time 55.640\n",
      "steps = 0004 | overall loss = 0.15220 | time 57.029\n",
      "steps = 0005 | overall loss = 0.14443 | time 55.713\n",
      "steps = 0006 | overall loss = 0.15261 | time 56.143\n",
      "steps = 0007 | overall loss = 0.13811 | time 51.924\n",
      "steps = 0008 | overall loss = 0.13400 | time 53.753\n",
      "steps = 0009 | overall loss = 0.13460 | time 56.937\n",
      "steps = 0010 | overall loss = 0.13012 | time 57.996\n",
      "steps = 0011 | overall loss = 0.12675 | time 54.856\n",
      "steps = 0012 | overall loss = 0.12516 | time 55.776\n",
      "steps = 0013 | overall loss = 0.11822 | time 55.304\n",
      "steps = 0014 | overall loss = 0.11837 | time 53.279\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.02\n",
    "batch_size = 7\n",
    "hidden_size = 128\n",
    "dtype = tf.float32\n",
    "\n",
    "with tf.variable_scope('fooo7', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [None, h, w, input_vec_size])\n",
    "    y = tf.placeholder(dtype, [None, h, w, output_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "    grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    epochs = 10\n",
    "    step = 0\n",
    "    for batch in batches(data, batch_size, epochs):\n",
    "        grad_step_start_time = time()\n",
    "\n",
    "        model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "            x: np.stack([x[0] for x in batch]),\n",
    "            y: np.stack([x[1] for x in batch]),\n",
    "        })\n",
    "\n",
    "        print('steps = {0} | overall loss = {1:.5f} | time {2:.3f}'.format(\n",
    "            str(step).zfill(4),\n",
    "            tot_loss_value,\n",
    "            time() - grad_step_start_time))\n",
    "\n",
    "        if tot_loss_value != tot_loss_value:\n",
    "            break\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.22644 | time 64.649\n",
      "steps = 0001 | overall loss = 0.16291 | time 56.049\n",
      "steps = 0002 | overall loss = 0.14643 | time 56.156\n",
      "steps = 0003 | overall loss = 0.12551 | time 55.817\n",
      "steps = 0004 | overall loss = 0.12539 | time 55.278\n",
      "steps = 0005 | overall loss = 0.12358 | time 57.295\n",
      "steps = 0006 | overall loss = 0.11688 | time 57.461\n",
      "steps = 0007 | overall loss = 0.11106 | time 56.348\n",
      "steps = 0008 | overall loss = 0.10781 | time 57.328\n",
      "steps = 0009 | overall loss = 0.09102 | time 59.377\n",
      "steps = 0010 | overall loss = 0.08235 | time 57.198\n",
      "steps = 0011 | overall loss = 0.07054 | time 58.629\n",
      "steps = 0012 | overall loss = 0.06678 | time 57.600\n",
      "steps = 0013 | overall loss = 0.05754 | time 58.708\n",
      "steps = 0014 | overall loss = 0.05704 | time 59.194\n",
      "steps = 0015 | overall loss = 0.04834 | time 58.255\n",
      "steps = 0016 | overall loss = 0.05195 | time 58.996\n",
      "steps = 0017 | overall loss = 0.04299 | time 58.363\n",
      "steps = 0018 | overall loss = 0.04293 | time 58.836\n",
      "steps = 0019 | overall loss = 0.04654 | time 56.893\n",
      "steps = 0020 | overall loss = 0.03287 | time 57.966\n",
      "steps = 0021 | overall loss = 0.04300 | time 57.549\n",
      "steps = 0022 | overall loss = 0.03948 | time 59.218\n",
      "steps = 0023 | overall loss = 0.03554 | time 58.167\n",
      "steps = 0024 | overall loss = 0.03450 | time 58.865\n",
      "steps = 0025 | overall loss = 0.03942 | time 58.690\n",
      "steps = 0026 | overall loss = 0.03186 | time 58.721\n",
      "steps = 0027 | overall loss = 0.03672 | time 58.206\n",
      "steps = 0028 | overall loss = 0.03378 | time 58.339\n",
      "steps = 0029 | overall loss = 0.03133 | time 58.269\n",
      "steps = 0030 | overall loss = 0.03448 | time 58.955\n",
      "steps = 0031 | overall loss = 0.03272 | time 58.431\n",
      "steps = 0032 | overall loss = 0.02855 | time 58.051\n",
      "steps = 0033 | overall loss = 0.02959 | time 58.142\n",
      "steps = 0034 | overall loss = 0.03146 | time 58.027\n",
      "steps = 0035 | overall loss = 0.03058 | time 54.457\n",
      "steps = 0036 | overall loss = 0.03043 | time 53.770\n",
      "steps = 0037 | overall loss = 0.02725 | time 54.569\n",
      "steps = 0038 | overall loss = 0.02995 | time 54.002\n",
      "steps = 0039 | overall loss = 0.03582 | time 54.432\n",
      "steps = 0040 | overall loss = 0.02686 | time 58.494\n",
      "steps = 0041 | overall loss = 0.02863 | time 58.929\n",
      "steps = 0042 | overall loss = 0.02618 | time 57.944\n",
      "steps = 0043 | overall loss = 0.03154 | time 59.286\n",
      "steps = 0044 | overall loss = 0.02821 | time 57.198\n",
      "steps = 0045 | overall loss = 0.02691 | time 57.842\n",
      "steps = 0046 | overall loss = 0.02578 | time 58.104\n",
      "steps = 0047 | overall loss = 0.02515 | time 58.690\n",
      "steps = 0048 | overall loss = 0.02399 | time 57.873\n",
      "steps = 0049 | overall loss = 0.02638 | time 59.216\n",
      "steps = 0050 | overall loss = 0.02461 | time 59.025\n",
      "steps = 0051 | overall loss = 0.02274 | time 61.556\n",
      "steps = 0052 | overall loss = 0.02635 | time 62.126\n",
      "steps = 0053 | overall loss = 0.03665 | time 60.396\n",
      "steps = 0054 | overall loss = 0.03699 | time 62.276\n",
      "steps = 0055 | overall loss = 0.04394 | time 61.528\n",
      "steps = 0056 | overall loss = 0.04039 | time 62.653\n",
      "steps = 0057 | overall loss = 0.02976 | time 62.144\n",
      "steps = 0058 | overall loss = 0.04528 | time 61.625\n",
      "steps = 0059 | overall loss = 0.04628 | time 61.613\n",
      "steps = 0060 | overall loss = 0.03677 | time 62.129\n",
      "steps = 0061 | overall loss = 0.03403 | time 61.353\n",
      "steps = 0062 | overall loss = 0.04017 | time 62.313\n",
      "steps = 0063 | overall loss = 0.04119 | time 60.861\n",
      "steps = 0064 | overall loss = 0.04520 | time 60.843\n",
      "steps = 0065 | overall loss = 0.03379 | time 62.103\n",
      "steps = 0066 | overall loss = 0.03918 | time 62.334\n",
      "steps = 0067 | overall loss = 0.03129 | time 61.915\n",
      "steps = 0068 | overall loss = 0.04448 | time 60.564\n",
      "steps = 0069 | overall loss = 0.03399 | time 61.630\n",
      "steps = 0070 | overall loss = 0.03697 | time 61.523\n",
      "steps = 0071 | overall loss = 0.03588 | time 61.623\n",
      "steps = 0072 | overall loss = 0.03151 | time 61.929\n",
      "steps = 0073 | overall loss = 0.04431 | time 61.840\n",
      "steps = 0074 | overall loss = 0.02862 | time 61.780\n",
      "steps = 0075 | overall loss = 0.04239 | time 61.161\n",
      "steps = 0076 | overall loss = 0.03509 | time 62.159\n",
      "steps = 0077 | overall loss = 0.04105 | time 61.777\n",
      "steps = 0078 | overall loss = 0.02871 | time 61.678\n",
      "steps = 0079 | overall loss = 0.03108 | time 61.668\n",
      "steps = 0080 | overall loss = 0.04043 | time 61.477\n",
      "steps = 0081 | overall loss = 0.03530 | time 62.470\n",
      "steps = 0082 | overall loss = 0.03463 | time 60.516\n",
      "steps = 0083 | overall loss = 0.02461 | time 62.451\n",
      "steps = 0084 | overall loss = 0.03979 | time 61.959\n",
      "steps = 0085 | overall loss = 0.03609 | time 62.405\n",
      "steps = 0086 | overall loss = 0.03459 | time 62.045\n",
      "steps = 0087 | overall loss = 0.03321 | time 62.251\n",
      "steps = 0088 | overall loss = 0.03304 | time 61.774\n",
      "steps = 0089 | overall loss = 0.03270 | time 60.182\n",
      "steps = 0090 | overall loss = 0.03757 | time 62.021\n",
      "steps = 0091 | overall loss = 0.03060 | time 62.346\n",
      "steps = 0092 | overall loss = 0.03771 | time 61.202\n",
      "steps = 0093 | overall loss = 0.03533 | time 61.304\n",
      "steps = 0094 | overall loss = 0.02657 | time 61.741\n",
      "steps = 0095 | overall loss = 0.03255 | time 62.020\n",
      "steps = 0096 | overall loss = 0.02506 | time 60.650\n",
      "steps = 0097 | overall loss = 0.04579 | time 62.250\n",
      "steps = 0098 | overall loss = 0.03093 | time 60.759\n",
      "steps = 0099 | overall loss = 0.03210 | time 61.865\n",
      "steps = 0100 | overall loss = 0.03729 | time 61.969\n",
      "steps = 0101 | overall loss = 0.02788 | time 63.898\n",
      "steps = 0102 | overall loss = 0.02873 | time 62.587\n",
      "steps = 0103 | overall loss = 0.03145 | time 63.876\n",
      "steps = 0104 | overall loss = 0.03322 | time 64.003\n",
      "steps = 0105 | overall loss = 0.02420 | time 63.681\n",
      "steps = 0106 | overall loss = 0.02955 | time 63.623\n",
      "steps = 0107 | overall loss = 0.02955 | time 63.387\n",
      "steps = 0108 | overall loss = 0.02874 | time 64.094\n",
      "steps = 0109 | overall loss = 0.02768 | time 64.000\n",
      "steps = 0110 | overall loss = 0.03026 | time 63.562\n",
      "steps = 0111 | overall loss = 0.02460 | time 63.888\n",
      "steps = 0112 | overall loss = 0.02769 | time 63.040\n",
      "steps = 0113 | overall loss = 0.02518 | time 63.928\n",
      "steps = 0114 | overall loss = 0.02607 | time 63.976\n",
      "steps = 0115 | overall loss = 0.02648 | time 63.952\n",
      "steps = 0116 | overall loss = 0.02552 | time 63.814\n",
      "steps = 0117 | overall loss = 0.02721 | time 63.552\n",
      "steps = 0118 | overall loss = 0.02474 | time 64.439\n",
      "steps = 0119 | overall loss = 0.02709 | time 64.069\n",
      "steps = 0120 | overall loss = 0.02552 | time 63.733\n",
      "steps = 0121 | overall loss = 0.02289 | time 63.442\n",
      "steps = 0122 | overall loss = 0.02931 | time 62.248\n",
      "steps = 0123 | overall loss = 0.02267 | time 64.211\n",
      "steps = 0124 | overall loss = 0.02492 | time 63.532\n",
      "steps = 0125 | overall loss = 0.02382 | time 63.723\n",
      "steps = 0126 | overall loss = 0.02121 | time 63.000\n",
      "steps = 0127 | overall loss = 0.02541 | time 63.013\n",
      "steps = 0128 | overall loss = 0.02171 | time 63.685\n",
      "steps = 0129 | overall loss = 0.02182 | time 63.969\n",
      "steps = 0130 | overall loss = 0.02102 | time 64.189\n",
      "steps = 0131 | overall loss = 0.02388 | time 63.625\n",
      "steps = 0132 | overall loss = 0.02182 | time 64.123\n",
      "steps = 0133 | overall loss = 0.01979 | time 63.972\n",
      "steps = 0134 | overall loss = 0.02305 | time 64.012\n",
      "steps = 0135 | overall loss = 0.02010 | time 63.167\n",
      "steps = 0136 | overall loss = 0.02346 | time 64.051\n",
      "steps = 0137 | overall loss = 0.01861 | time 64.036\n",
      "steps = 0138 | overall loss = 0.02194 | time 64.105\n",
      "steps = 0139 | overall loss = 0.01914 | time 63.149\n",
      "steps = 0140 | overall loss = 0.02114 | time 61.856\n",
      "steps = 0141 | overall loss = 0.02228 | time 63.727\n",
      "steps = 0142 | overall loss = 0.01959 | time 63.061\n",
      "steps = 0143 | overall loss = 0.02144 | time 61.697\n",
      "steps = 0144 | overall loss = 0.02228 | time 63.139\n",
      "steps = 0145 | overall loss = 0.02072 | time 62.509\n",
      "steps = 0146 | overall loss = 0.02082 | time 63.477\n",
      "steps = 0147 | overall loss = 0.02288 | time 63.719\n",
      "steps = 0148 | overall loss = 0.02290 | time 63.613\n",
      "steps = 0149 | overall loss = 0.01863 | time 62.514\n",
      "steps = 0150 | overall loss = 0.02248 | time 63.423\n",
      "steps = 0151 | overall loss = 0.01972 | time 64.910\n",
      "steps = 0152 | overall loss = 0.02091 | time 65.036\n",
      "steps = 0153 | overall loss = 0.01843 | time 65.302\n",
      "steps = 0154 | overall loss = 0.02068 | time 64.998\n",
      "steps = 0155 | overall loss = 0.01812 | time 65.587\n",
      "steps = 0156 | overall loss = 0.01968 | time 65.462\n",
      "steps = 0157 | overall loss = 0.02203 | time 65.687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0158 | overall loss = 0.01978 | time 65.793\n",
      "steps = 0159 | overall loss = 0.01960 | time 65.433\n",
      "steps = 0160 | overall loss = 0.01792 | time 65.900\n",
      "steps = 0161 | overall loss = 0.01871 | time 65.889\n",
      "steps = 0162 | overall loss = 0.01761 | time 65.529\n",
      "steps = 0163 | overall loss = 0.02157 | time 65.062\n",
      "steps = 0164 | overall loss = 0.01884 | time 65.299\n",
      "steps = 0165 | overall loss = 0.01892 | time 65.680\n",
      "steps = 0166 | overall loss = 0.02081 | time 64.583\n",
      "steps = 0167 | overall loss = 0.01742 | time 65.877\n",
      "steps = 0168 | overall loss = 0.01832 | time 65.148\n",
      "steps = 0169 | overall loss = 0.01957 | time 65.725\n",
      "steps = 0170 | overall loss = 0.01868 | time 65.266\n",
      "steps = 0171 | overall loss = 0.01644 | time 65.258\n",
      "steps = 0172 | overall loss = 0.02026 | time 65.048\n",
      "steps = 0173 | overall loss = 0.01794 | time 65.331\n",
      "steps = 0174 | overall loss = 0.01705 | time 65.352\n",
      "steps = 0175 | overall loss = 0.01883 | time 65.036\n",
      "steps = 0176 | overall loss = 0.01987 | time 65.689\n",
      "steps = 0177 | overall loss = 0.01643 | time 65.099\n",
      "steps = 0178 | overall loss = 0.01787 | time 65.647\n",
      "steps = 0179 | overall loss = 0.01716 | time 64.567\n",
      "steps = 0180 | overall loss = 0.01936 | time 65.482\n",
      "steps = 0181 | overall loss = 0.01809 | time 65.611\n",
      "steps = 0182 | overall loss = 0.01482 | time 65.801\n",
      "steps = 0183 | overall loss = 0.02011 | time 64.942\n",
      "steps = 0184 | overall loss = 0.01749 | time 65.288\n",
      "steps = 0185 | overall loss = 0.01646 | time 65.724\n",
      "steps = 0186 | overall loss = 0.01775 | time 65.177\n",
      "steps = 0187 | overall loss = 0.01765 | time 65.764\n",
      "steps = 0188 | overall loss = 0.01689 | time 65.289\n",
      "steps = 0189 | overall loss = 0.01771 | time 65.386\n",
      "steps = 0190 | overall loss = 0.01589 | time 65.328\n",
      "steps = 0191 | overall loss = 0.01869 | time 65.263\n",
      "steps = 0192 | overall loss = 0.01571 | time 65.380\n",
      "steps = 0193 | overall loss = 0.01946 | time 65.577\n",
      "steps = 0194 | overall loss = 0.01639 | time 64.569\n",
      "steps = 0195 | overall loss = 0.01574 | time 64.966\n",
      "steps = 0196 | overall loss = 0.01547 | time 65.294\n",
      "steps = 0197 | overall loss = 0.01788 | time 65.274\n",
      "steps = 0198 | overall loss = 0.01435 | time 65.488\n",
      "steps = 0199 | overall loss = 0.01771 | time 65.194\n",
      "steps = 0200 | overall loss = 0.01792 | time 65.546\n",
      "steps = 0201 | overall loss = 0.01702 | time 66.996\n",
      "steps = 0202 | overall loss = 0.01536 | time 65.997\n",
      "steps = 0203 | overall loss = 0.01774 | time 66.027\n",
      "steps = 0204 | overall loss = 0.01501 | time 66.009\n",
      "steps = 0205 | overall loss = 0.01643 | time 67.014\n",
      "steps = 0206 | overall loss = 0.01765 | time 66.002\n",
      "steps = 0207 | overall loss = 0.01598 | time 65.855\n",
      "steps = 0208 | overall loss = 0.01602 | time 66.184\n",
      "steps = 0209 | overall loss = 0.01419 | time 66.692\n",
      "steps = 0210 | overall loss = 0.01830 | time 66.793\n",
      "steps = 0211 | overall loss = 0.01531 | time 65.433\n",
      "steps = 0212 | overall loss = 0.01662 | time 66.331\n",
      "steps = 0213 | overall loss = 0.01567 | time 66.479\n",
      "steps = 0214 | overall loss = 0.01659 | time 66.792\n",
      "steps = 0215 | overall loss = 0.01636 | time 64.341\n",
      "steps = 0216 | overall loss = 0.01437 | time 61.992\n",
      "steps = 0217 | overall loss = 0.01609 | time 61.503\n",
      "steps = 0218 | overall loss = 0.01657 | time 61.598\n",
      "steps = 0219 | overall loss = 0.01511 | time 63.941\n",
      "steps = 0220 | overall loss = 0.01643 | time 66.344\n",
      "steps = 0221 | overall loss = 0.01487 | time 66.305\n",
      "steps = 0222 | overall loss = 0.01475 | time 66.766\n",
      "steps = 0223 | overall loss = 0.01847 | time 66.894\n",
      "steps = 0224 | overall loss = 0.01359 | time 66.479\n",
      "steps = 0225 | overall loss = 0.01667 | time 66.532\n",
      "steps = 0226 | overall loss = 0.01545 | time 67.018\n",
      "steps = 0227 | overall loss = 0.01608 | time 66.845\n",
      "steps = 0228 | overall loss = 0.01546 | time 66.471\n",
      "steps = 0229 | overall loss = 0.01630 | time 66.400\n",
      "steps = 0230 | overall loss = 0.01423 | time 66.307\n",
      "steps = 0231 | overall loss = 0.01573 | time 66.471\n",
      "steps = 0232 | overall loss = 0.01488 | time 65.878\n",
      "steps = 0233 | overall loss = 0.01546 | time 65.991\n",
      "steps = 0234 | overall loss = 0.01528 | time 66.710\n",
      "steps = 0235 | overall loss = 0.01422 | time 66.919\n",
      "steps = 0236 | overall loss = 0.01658 | time 66.422\n",
      "steps = 0237 | overall loss = 0.01329 | time 66.006\n",
      "steps = 0238 | overall loss = 0.01745 | time 66.795\n",
      "steps = 0239 | overall loss = 0.01534 | time 66.276\n",
      "steps = 0240 | overall loss = 0.01359 | time 66.824\n",
      "steps = 0241 | overall loss = 0.01658 | time 66.770\n",
      "steps = 0242 | overall loss = 0.01349 | time 66.885\n",
      "steps = 0243 | overall loss = 0.01606 | time 66.375\n",
      "steps = 0244 | overall loss = 0.01587 | time 66.048\n",
      "steps = 0245 | overall loss = 0.01458 | time 66.281\n",
      "steps = 0246 | overall loss = 0.01486 | time 66.299\n",
      "steps = 0247 | overall loss = 0.01605 | time 66.730\n",
      "steps = 0248 | overall loss = 0.01326 | time 66.262\n",
      "steps = 0249 | overall loss = 0.01515 | time 65.562\n",
      "steps = 0250 | overall loss = 0.01334 | time 66.258\n",
      "steps = 0251 | overall loss = 0.01597 | time 67.221\n",
      "steps = 0252 | overall loss = 0.01517 | time 67.078\n",
      "steps = 0253 | overall loss = 0.01333 | time 66.789\n",
      "steps = 0254 | overall loss = 0.01610 | time 67.094\n",
      "steps = 0255 | overall loss = 0.01504 | time 67.298\n",
      "steps = 0256 | overall loss = 0.01399 | time 66.468\n",
      "steps = 0257 | overall loss = 0.01593 | time 67.305\n",
      "steps = 0258 | overall loss = 0.01363 | time 66.506\n",
      "steps = 0259 | overall loss = 0.01513 | time 67.030\n",
      "steps = 0260 | overall loss = 0.01265 | time 66.936\n",
      "steps = 0261 | overall loss = 0.01548 | time 66.221\n",
      "steps = 0262 | overall loss = 0.01393 | time 67.635\n",
      "steps = 0263 | overall loss = 0.01391 | time 67.137\n",
      "steps = 0264 | overall loss = 0.01488 | time 67.588\n",
      "steps = 0265 | overall loss = 0.01337 | time 66.819\n",
      "steps = 0266 | overall loss = 0.01485 | time 67.380\n",
      "steps = 0267 | overall loss = 0.01320 | time 67.016\n",
      "steps = 0268 | overall loss = 0.01456 | time 67.413\n",
      "steps = 0269 | overall loss = 0.01361 | time 67.224\n",
      "steps = 0270 | overall loss = 0.01555 | time 67.083\n",
      "steps = 0271 | overall loss = 0.01501 | time 66.779\n",
      "steps = 0272 | overall loss = 0.01435 | time 67.146\n",
      "steps = 0273 | overall loss = 0.01561 | time 67.285\n",
      "steps = 0274 | overall loss = 0.01289 | time 66.719\n",
      "steps = 0275 | overall loss = 0.01469 | time 67.469\n",
      "steps = 0276 | overall loss = 0.01647 | time 66.611\n",
      "steps = 0277 | overall loss = 0.01556 | time 67.040\n",
      "steps = 0278 | overall loss = 0.01523 | time 66.996\n",
      "steps = 0279 | overall loss = 0.01593 | time 66.868\n",
      "steps = 0280 | overall loss = 0.01429 | time 66.784\n",
      "steps = 0281 | overall loss = 0.01542 | time 67.027\n",
      "steps = 0282 | overall loss = 0.01573 | time 67.486\n",
      "steps = 0283 | overall loss = 0.01538 | time 67.035\n",
      "steps = 0284 | overall loss = 0.01714 | time 67.669\n",
      "steps = 0285 | overall loss = 0.01515 | time 66.338\n",
      "steps = 0286 | overall loss = 0.01616 | time 67.188\n",
      "steps = 0287 | overall loss = 0.01593 | time 66.842\n",
      "steps = 0288 | overall loss = 0.01716 | time 66.839\n",
      "steps = 0289 | overall loss = 0.01425 | time 67.257\n",
      "steps = 0290 | overall loss = 0.01804 | time 67.228\n",
      "steps = 0291 | overall loss = 0.01624 | time 66.665\n",
      "steps = 0292 | overall loss = 0.01405 | time 66.968\n",
      "steps = 0293 | overall loss = 0.01583 | time 67.396\n",
      "steps = 0294 | overall loss = 0.01620 | time 67.048\n",
      "steps = 0295 | overall loss = 0.01658 | time 67.219\n",
      "steps = 0296 | overall loss = 0.01417 | time 67.099\n",
      "steps = 0297 | overall loss = 0.01712 | time 65.584\n",
      "steps = 0298 | overall loss = 0.01260 | time 67.257\n",
      "steps = 0299 | overall loss = 0.01564 | time 67.175\n",
      "steps = 0300 | overall loss = 0.01597 | time 67.215\n",
      "steps = 0301 | overall loss = 0.01349 | time 67.577\n",
      "steps = 0302 | overall loss = 0.01402 | time 66.967\n",
      "steps = 0303 | overall loss = 0.01334 | time 67.213\n",
      "steps = 0304 | overall loss = 0.01564 | time 67.315\n",
      "steps = 0305 | overall loss = 0.01270 | time 67.001\n",
      "steps = 0306 | overall loss = 0.01661 | time 66.996\n",
      "steps = 0307 | overall loss = 0.01326 | time 66.966\n",
      "steps = 0308 | overall loss = 0.01442 | time 67.178\n",
      "steps = 0309 | overall loss = 0.01382 | time 66.382\n",
      "steps = 0310 | overall loss = 0.01321 | time 66.822\n",
      "steps = 0311 | overall loss = 0.01473 | time 66.804\n",
      "steps = 0312 | overall loss = 0.01194 | time 66.858\n",
      "steps = 0313 | overall loss = 0.01521 | time 66.983\n",
      "steps = 0314 | overall loss = 0.01257 | time 66.630\n",
      "steps = 0315 | overall loss = 0.01281 | time 66.894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0316 | overall loss = 0.01484 | time 67.048\n",
      "steps = 0317 | overall loss = 0.01287 | time 66.421\n",
      "steps = 0318 | overall loss = 0.01302 | time 66.899\n",
      "steps = 0319 | overall loss = 0.01422 | time 67.288\n",
      "steps = 0320 | overall loss = 0.01320 | time 67.195\n",
      "steps = 0321 | overall loss = 0.01245 | time 67.322\n",
      "steps = 0322 | overall loss = 0.01333 | time 67.174\n",
      "steps = 0323 | overall loss = 0.01286 | time 66.961\n",
      "steps = 0324 | overall loss = 0.01325 | time 67.120\n",
      "steps = 0325 | overall loss = 0.01213 | time 66.502\n",
      "steps = 0326 | overall loss = 0.01264 | time 67.307\n",
      "steps = 0327 | overall loss = 0.01210 | time 66.844\n",
      "steps = 0328 | overall loss = 0.01219 | time 67.160\n",
      "steps = 0329 | overall loss = 0.01470 | time 66.710\n",
      "steps = 0330 | overall loss = 0.01379 | time 67.027\n",
      "steps = 0331 | overall loss = 0.01115 | time 67.292\n",
      "steps = 0332 | overall loss = 0.01271 | time 66.282\n",
      "steps = 0333 | overall loss = 0.01341 | time 66.818\n",
      "steps = 0334 | overall loss = 0.01204 | time 66.778\n",
      "steps = 0335 | overall loss = 0.01215 | time 67.270\n",
      "steps = 0336 | overall loss = 0.01539 | time 67.273\n",
      "steps = 0337 | overall loss = 0.01125 | time 67.482\n",
      "steps = 0338 | overall loss = 0.01258 | time 66.533\n",
      "steps = 0339 | overall loss = 0.01175 | time 67.164\n",
      "steps = 0340 | overall loss = 0.01267 | time 66.956\n",
      "steps = 0341 | overall loss = 0.01127 | time 66.437\n",
      "steps = 0342 | overall loss = 0.01325 | time 67.266\n",
      "steps = 0343 | overall loss = 0.01296 | time 66.664\n",
      "steps = 0344 | overall loss = 0.01108 | time 66.702\n",
      "steps = 0345 | overall loss = 0.01332 | time 67.070\n",
      "steps = 0346 | overall loss = 0.01311 | time 67.433\n",
      "steps = 0347 | overall loss = 0.01226 | time 67.141\n",
      "steps = 0348 | overall loss = 0.01205 | time 67.408\n",
      "steps = 0349 | overall loss = 0.01128 | time 67.206\n",
      "steps = 0350 | overall loss = 0.01512 | time 66.767\n",
      "steps = 0351 | overall loss = 0.01153 | time 67.205\n",
      "steps = 0352 | overall loss = 0.01234 | time 67.394\n",
      "steps = 0353 | overall loss = 0.01349 | time 67.181\n",
      "steps = 0354 | overall loss = 0.01190 | time 67.769\n",
      "steps = 0355 | overall loss = 0.01184 | time 67.161\n",
      "steps = 0356 | overall loss = 0.01233 | time 67.479\n",
      "steps = 0357 | overall loss = 0.01168 | time 67.058\n",
      "steps = 0358 | overall loss = 0.01330 | time 67.297\n",
      "steps = 0359 | overall loss = 0.01137 | time 66.973\n",
      "steps = 0360 | overall loss = 0.01268 | time 67.235\n",
      "steps = 0361 | overall loss = 0.01109 | time 67.103\n",
      "steps = 0362 | overall loss = 0.01251 | time 66.257\n",
      "steps = 0363 | overall loss = 0.01225 | time 67.335\n",
      "steps = 0364 | overall loss = 0.01184 | time 67.399\n",
      "steps = 0365 | overall loss = 0.01195 | time 67.345\n",
      "steps = 0366 | overall loss = 0.01124 | time 67.142\n",
      "steps = 0367 | overall loss = 0.01199 | time 67.024\n",
      "steps = 0368 | overall loss = 0.01173 | time 67.070\n",
      "steps = 0369 | overall loss = 0.01116 | time 67.522\n",
      "steps = 0370 | overall loss = 0.01121 | time 67.249\n",
      "steps = 0371 | overall loss = 0.01182 | time 67.110\n",
      "steps = 0372 | overall loss = 0.01007 | time 66.878\n",
      "steps = 0373 | overall loss = 0.01218 | time 67.064\n",
      "steps = 0374 | overall loss = 0.01113 | time 67.512\n",
      "steps = 0375 | overall loss = 0.01135 | time 66.707\n",
      "steps = 0376 | overall loss = 0.01152 | time 67.245\n",
      "steps = 0377 | overall loss = 0.01074 | time 67.036\n",
      "steps = 0378 | overall loss = 0.01235 | time 67.477\n",
      "steps = 0379 | overall loss = 0.01438 | time 67.690\n",
      "steps = 0380 | overall loss = 0.01360 | time 66.131\n",
      "steps = 0381 | overall loss = 0.01301 | time 67.250\n",
      "steps = 0382 | overall loss = 0.01342 | time 67.112\n",
      "steps = 0383 | overall loss = 0.01198 | time 66.769\n",
      "steps = 0384 | overall loss = 0.01383 | time 67.404\n",
      "steps = 0385 | overall loss = 0.01411 | time 67.119\n",
      "steps = 0386 | overall loss = 0.01178 | time 67.157\n",
      "steps = 0387 | overall loss = 0.01353 | time 63.812\n",
      "steps = 0388 | overall loss = 0.01283 | time 65.287\n",
      "steps = 0389 | overall loss = 0.01189 | time 66.753\n",
      "steps = 0390 | overall loss = 0.01331 | time 67.592\n",
      "steps = 0391 | overall loss = 0.01338 | time 66.716\n",
      "steps = 0392 | overall loss = 0.01135 | time 66.924\n",
      "steps = 0393 | overall loss = 0.01343 | time 67.199\n",
      "steps = 0394 | overall loss = 0.01247 | time 67.132\n",
      "steps = 0395 | overall loss = 0.01229 | time 67.469\n",
      "steps = 0396 | overall loss = 0.01177 | time 67.026\n",
      "steps = 0397 | overall loss = 0.01117 | time 66.974\n",
      "steps = 0398 | overall loss = 0.01339 | time 67.037\n",
      "steps = 0399 | overall loss = 0.01193 | time 67.117\n",
      "steps = 0400 | overall loss = 0.01058 | time 66.984\n",
      "steps = 0401 | overall loss = 0.01281 | time 67.429\n",
      "steps = 0402 | overall loss = 0.01114 | time 67.181\n",
      "steps = 0403 | overall loss = 0.01134 | time 67.767\n",
      "steps = 0404 | overall loss = 0.01151 | time 67.110\n",
      "steps = 0405 | overall loss = 0.01134 | time 67.869\n",
      "steps = 0406 | overall loss = 0.01195 | time 67.439\n",
      "steps = 0407 | overall loss = 0.01074 | time 67.441\n",
      "steps = 0408 | overall loss = 0.01130 | time 67.368\n",
      "steps = 0409 | overall loss = 0.01232 | time 67.945\n",
      "steps = 0410 | overall loss = 0.01145 | time 67.614\n",
      "steps = 0411 | overall loss = 0.01011 | time 66.856\n",
      "steps = 0412 | overall loss = 0.01135 | time 67.252\n",
      "steps = 0413 | overall loss = 0.01101 | time 67.680\n",
      "steps = 0414 | overall loss = 0.01068 | time 67.658\n",
      "steps = 0415 | overall loss = 0.01203 | time 66.364\n",
      "steps = 0416 | overall loss = 0.00946 | time 67.468\n",
      "steps = 0417 | overall loss = 0.01124 | time 68.305\n",
      "steps = 0418 | overall loss = 0.01047 | time 67.106\n",
      "steps = 0419 | overall loss = 0.01029 | time 66.391\n",
      "steps = 0420 | overall loss = 0.00991 | time 66.821\n",
      "steps = 0421 | overall loss = 0.01120 | time 67.214\n",
      "steps = 0422 | overall loss = 0.01053 | time 67.538\n",
      "steps = 0423 | overall loss = 0.00946 | time 67.808\n",
      "steps = 0424 | overall loss = 0.01100 | time 66.672\n",
      "steps = 0425 | overall loss = 0.01094 | time 67.102\n",
      "steps = 0426 | overall loss = 0.01009 | time 65.629\n",
      "steps = 0427 | overall loss = 0.01169 | time 66.346\n",
      "steps = 0428 | overall loss = 0.01156 | time 66.109\n",
      "steps = 0429 | overall loss = 0.01167 | time 66.740\n",
      "steps = 0430 | overall loss = 0.01004 | time 66.846\n",
      "steps = 0431 | overall loss = 0.01138 | time 66.691\n",
      "steps = 0432 | overall loss = 0.00988 | time 68.455\n",
      "steps = 0433 | overall loss = 0.01214 | time 66.610\n",
      "steps = 0434 | overall loss = 0.01136 | time 67.487\n",
      "steps = 0435 | overall loss = 0.01062 | time 67.882\n",
      "steps = 0436 | overall loss = 0.00978 | time 66.070\n",
      "steps = 0437 | overall loss = 0.01073 | time 67.640\n",
      "steps = 0438 | overall loss = 0.01009 | time 67.047\n",
      "steps = 0439 | overall loss = 0.01097 | time 67.636\n",
      "steps = 0440 | overall loss = 0.01012 | time 67.781\n",
      "steps = 0441 | overall loss = 0.01108 | time 66.998\n",
      "steps = 0442 | overall loss = 0.00992 | time 66.729\n",
      "steps = 0443 | overall loss = 0.01021 | time 67.766\n",
      "steps = 0444 | overall loss = 0.01059 | time 67.682\n",
      "steps = 0445 | overall loss = 0.00914 | time 67.630\n",
      "steps = 0446 | overall loss = 0.00941 | time 67.940\n",
      "steps = 0447 | overall loss = 0.01244 | time 67.487\n",
      "steps = 0448 | overall loss = 0.01012 | time 67.587\n",
      "steps = 0449 | overall loss = 0.00865 | time 67.716\n",
      "steps = 0450 | overall loss = 0.01146 | time 67.861\n",
      "steps = 0451 | overall loss = 0.00969 | time 67.946\n",
      "steps = 0452 | overall loss = 0.01059 | time 67.694\n",
      "steps = 0453 | overall loss = 0.01054 | time 67.763\n",
      "steps = 0454 | overall loss = 0.00835 | time 67.494\n",
      "steps = 0455 | overall loss = 0.01198 | time 66.555\n",
      "steps = 0456 | overall loss = 0.01093 | time 68.162\n",
      "steps = 0457 | overall loss = 0.00861 | time 67.367\n",
      "steps = 0458 | overall loss = 0.01249 | time 68.103\n",
      "steps = 0459 | overall loss = 0.01003 | time 67.465\n",
      "steps = 0460 | overall loss = 0.01019 | time 67.922\n",
      "steps = 0461 | overall loss = 0.01323 | time 67.506\n",
      "steps = 0462 | overall loss = 0.01091 | time 67.821\n",
      "steps = 0463 | overall loss = 0.01098 | time 67.736\n",
      "steps = 0464 | overall loss = 0.01158 | time 67.431\n",
      "steps = 0465 | overall loss = 0.01098 | time 67.307\n",
      "steps = 0466 | overall loss = 0.01009 | time 67.800\n",
      "steps = 0467 | overall loss = 0.01149 | time 67.815\n",
      "steps = 0468 | overall loss = 0.01210 | time 65.842\n",
      "steps = 0469 | overall loss = 0.01038 | time 67.222\n",
      "steps = 0470 | overall loss = 0.01133 | time 67.863\n",
      "steps = 0471 | overall loss = 0.01098 | time 67.024\n",
      "steps = 0472 | overall loss = 0.01069 | time 66.969\n",
      "steps = 0473 | overall loss = 0.01089 | time 67.033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0474 | overall loss = 0.01037 | time 68.786\n",
      "steps = 0475 | overall loss = 0.01051 | time 67.151\n",
      "steps = 0476 | overall loss = 0.01243 | time 67.063\n",
      "steps = 0477 | overall loss = 0.01090 | time 66.799\n",
      "steps = 0478 | overall loss = 0.01116 | time 67.801\n",
      "steps = 0479 | overall loss = 0.00976 | time 67.915\n",
      "steps = 0480 | overall loss = 0.01091 | time 67.641\n",
      "steps = 0481 | overall loss = 0.01234 | time 67.121\n",
      "steps = 0482 | overall loss = 0.00945 | time 68.052\n",
      "steps = 0483 | overall loss = 0.01155 | time 67.897\n",
      "steps = 0484 | overall loss = 0.00928 | time 67.302\n",
      "steps = 0485 | overall loss = 0.01264 | time 67.783\n",
      "steps = 0486 | overall loss = 0.00991 | time 67.229\n",
      "steps = 0487 | overall loss = 0.01197 | time 65.058\n",
      "steps = 0488 | overall loss = 0.00840 | time 64.647\n",
      "steps = 0489 | overall loss = 0.01039 | time 64.057\n",
      "steps = 0490 | overall loss = 0.01004 | time 65.392\n",
      "steps = 0491 | overall loss = 0.01277 | time 65.324\n",
      "steps = 0492 | overall loss = 0.01017 | time 64.455\n",
      "steps = 0493 | overall loss = 0.01093 | time 63.553\n",
      "steps = 0494 | overall loss = 0.00932 | time 67.856\n",
      "steps = 0495 | overall loss = 0.00969 | time 66.566\n",
      "steps = 0496 | overall loss = 0.00989 | time 67.479\n",
      "steps = 0497 | overall loss = 0.00967 | time 67.133\n",
      "steps = 0498 | overall loss = 0.01166 | time 67.269\n",
      "steps = 0499 | overall loss = 0.00874 | time 67.697\n",
      "steps = 0500 | overall loss = 0.01060 | time 66.680\n",
      "steps = 0501 | overall loss = 0.00965 | time 67.056\n",
      "steps = 0502 | overall loss = 0.00921 | time 66.956\n",
      "steps = 0503 | overall loss = 0.01053 | time 68.127\n",
      "steps = 0504 | overall loss = 0.00834 | time 68.194\n",
      "steps = 0505 | overall loss = 0.01030 | time 67.930\n",
      "steps = 0506 | overall loss = 0.00978 | time 68.163\n",
      "steps = 0507 | overall loss = 0.00860 | time 67.582\n",
      "steps = 0508 | overall loss = 0.01013 | time 67.501\n",
      "steps = 0509 | overall loss = 0.00969 | time 67.728\n",
      "steps = 0510 | overall loss = 0.00789 | time 67.248\n",
      "steps = 0511 | overall loss = 0.01025 | time 67.678\n",
      "steps = 0512 | overall loss = 0.00963 | time 67.798\n",
      "steps = 0513 | overall loss = 0.00840 | time 68.031\n",
      "steps = 0514 | overall loss = 0.00827 | time 67.501\n",
      "steps = 0515 | overall loss = 0.01050 | time 67.706\n",
      "steps = 0516 | overall loss = 0.00850 | time 67.961\n",
      "steps = 0517 | overall loss = 0.00934 | time 67.611\n",
      "steps = 0518 | overall loss = 0.00771 | time 66.335\n",
      "steps = 0519 | overall loss = 0.00894 | time 65.199\n",
      "steps = 0520 | overall loss = 0.00910 | time 67.302\n",
      "steps = 0521 | overall loss = 0.00866 | time 65.433\n",
      "steps = 0522 | overall loss = 0.00985 | time 65.644\n",
      "steps = 0523 | overall loss = 0.00827 | time 65.502\n",
      "steps = 0524 | overall loss = 0.00861 | time 66.113\n",
      "steps = 0525 | overall loss = 0.00943 | time 66.323\n",
      "steps = 0526 | overall loss = 0.00875 | time 66.554\n",
      "steps = 0527 | overall loss = 0.00873 | time 66.713\n",
      "steps = 0528 | overall loss = 0.00879 | time 67.926\n",
      "steps = 0529 | overall loss = 0.00813 | time 67.253\n",
      "steps = 0530 | overall loss = 0.00931 | time 65.441\n",
      "steps = 0531 | overall loss = 0.00870 | time 68.291\n",
      "steps = 0532 | overall loss = 0.00766 | time 67.186\n",
      "steps = 0533 | overall loss = 0.00940 | time 67.106\n",
      "steps = 0534 | overall loss = 0.00720 | time 67.455\n",
      "steps = 0535 | overall loss = 0.00956 | time 66.985\n",
      "steps = 0536 | overall loss = 0.00907 | time 68.291\n",
      "steps = 0537 | overall loss = 0.00716 | time 67.621\n",
      "steps = 0538 | overall loss = 0.00916 | time 68.264\n",
      "steps = 0539 | overall loss = 0.00879 | time 67.441\n",
      "steps = 0540 | overall loss = 0.00755 | time 65.055\n",
      "steps = 0541 | overall loss = 0.00876 | time 67.058\n",
      "steps = 0542 | overall loss = 0.00860 | time 66.323\n",
      "steps = 0543 | overall loss = 0.00778 | time 67.912\n",
      "steps = 0544 | overall loss = 0.00848 | time 65.371\n",
      "steps = 0545 | overall loss = 0.00894 | time 66.528\n",
      "steps = 0546 | overall loss = 0.00890 | time 65.593\n",
      "steps = 0547 | overall loss = 0.00787 | time 65.710\n",
      "steps = 0548 | overall loss = 0.00821 | time 68.205\n",
      "steps = 0549 | overall loss = 0.00924 | time 69.010\n",
      "steps = 0550 | overall loss = 0.00837 | time 68.457\n",
      "steps = 0551 | overall loss = 0.00786 | time 67.473\n",
      "steps = 0552 | overall loss = 0.00918 | time 67.769\n",
      "steps = 0553 | overall loss = 0.00822 | time 66.150\n",
      "steps = 0554 | overall loss = 0.00777 | time 63.618\n",
      "steps = 0555 | overall loss = 0.00877 | time 65.338\n",
      "steps = 0556 | overall loss = 0.00882 | time 66.669\n",
      "steps = 0557 | overall loss = 0.00834 | time 68.835\n",
      "steps = 0558 | overall loss = 0.00949 | time 68.750\n",
      "steps = 0559 | overall loss = 0.00875 | time 68.970\n",
      "steps = 0560 | overall loss = 0.00814 | time 68.059\n",
      "steps = 0561 | overall loss = 0.00861 | time 67.707\n",
      "steps = 0562 | overall loss = 0.00802 | time 69.021\n",
      "steps = 0563 | overall loss = 0.00962 | time 68.900\n",
      "steps = 0564 | overall loss = 0.00837 | time 68.204\n",
      "steps = 0565 | overall loss = 0.00923 | time 66.458\n",
      "steps = 0566 | overall loss = 0.00987 | time 65.462\n",
      "steps = 0567 | overall loss = 0.00851 | time 67.055\n",
      "steps = 0568 | overall loss = 0.00926 | time 64.253\n",
      "steps = 0569 | overall loss = 0.01025 | time 67.983\n",
      "steps = 0570 | overall loss = 0.00763 | time 67.240\n",
      "steps = 0571 | overall loss = 0.01038 | time 67.646\n",
      "steps = 0572 | overall loss = 0.00897 | time 68.074\n",
      "steps = 0573 | overall loss = 0.00915 | time 68.238\n",
      "steps = 0574 | overall loss = 0.00831 | time 67.497\n",
      "steps = 0575 | overall loss = 0.01072 | time 67.739\n",
      "steps = 0576 | overall loss = 0.00840 | time 68.028\n",
      "steps = 0577 | overall loss = 0.00850 | time 67.589\n",
      "steps = 0578 | overall loss = 0.00993 | time 67.089\n",
      "steps = 0579 | overall loss = 0.00839 | time 67.302\n",
      "steps = 0580 | overall loss = 0.00988 | time 67.433\n",
      "steps = 0581 | overall loss = 0.00812 | time 66.960\n",
      "steps = 0582 | overall loss = 0.01005 | time 67.980\n",
      "steps = 0583 | overall loss = 0.00964 | time 67.804\n",
      "steps = 0584 | overall loss = 0.00859 | time 66.916\n",
      "steps = 0585 | overall loss = 0.00777 | time 67.396\n",
      "steps = 0586 | overall loss = 0.00886 | time 67.718\n",
      "steps = 0587 | overall loss = 0.00829 | time 66.917\n",
      "steps = 0588 | overall loss = 0.00820 | time 67.859\n",
      "steps = 0589 | overall loss = 0.00835 | time 67.326\n",
      "steps = 0590 | overall loss = 0.00733 | time 67.599\n",
      "steps = 0591 | overall loss = 0.00845 | time 67.965\n",
      "steps = 0592 | overall loss = 0.00741 | time 65.974\n",
      "steps = 0593 | overall loss = 0.00858 | time 66.551\n",
      "steps = 0594 | overall loss = 0.00771 | time 65.468\n",
      "steps = 0595 | overall loss = 0.00811 | time 65.846\n",
      "steps = 0596 | overall loss = 0.00770 | time 67.085\n",
      "steps = 0597 | overall loss = 0.00782 | time 64.150\n",
      "steps = 0598 | overall loss = 0.00953 | time 65.228\n",
      "steps = 0599 | overall loss = 0.00733 | time 65.295\n",
      "steps = 0600 | overall loss = 0.00911 | time 64.370\n",
      "steps = 0601 | overall loss = 0.00851 | time 64.948\n",
      "steps = 0602 | overall loss = 0.00717 | time 66.421\n",
      "steps = 0603 | overall loss = 0.00890 | time 65.216\n",
      "steps = 0604 | overall loss = 0.00909 | time 65.458\n",
      "steps = 0605 | overall loss = 0.00885 | time 67.903\n",
      "steps = 0606 | overall loss = 0.00761 | time 68.353\n",
      "steps = 0607 | overall loss = 0.00873 | time 68.496\n",
      "steps = 0608 | overall loss = 0.01058 | time 68.490\n",
      "steps = 0609 | overall loss = 0.00821 | time 68.436\n",
      "steps = 0610 | overall loss = 0.00790 | time 68.837\n",
      "steps = 0611 | overall loss = 0.01050 | time 68.138\n",
      "steps = 0612 | overall loss = 0.00894 | time 68.583\n",
      "steps = 0613 | overall loss = 0.00904 | time 67.488\n",
      "steps = 0614 | overall loss = 0.01069 | time 67.241\n",
      "steps = 0615 | overall loss = 0.00810 | time 67.312\n",
      "steps = 0616 | overall loss = 0.00890 | time 66.967\n",
      "steps = 0617 | overall loss = 0.00943 | time 67.355\n",
      "steps = 0618 | overall loss = 0.00876 | time 66.532\n",
      "steps = 0619 | overall loss = 0.00863 | time 67.389\n",
      "steps = 0620 | overall loss = 0.00803 | time 67.314\n",
      "steps = 0621 | overall loss = 0.00940 | time 66.773\n",
      "steps = 0622 | overall loss = 0.00827 | time 67.299\n",
      "steps = 0623 | overall loss = 0.00745 | time 67.418\n",
      "steps = 0624 | overall loss = 0.00893 | time 67.131\n",
      "steps = 0625 | overall loss = 0.00754 | time 69.039\n",
      "steps = 0626 | overall loss = 0.00825 | time 68.684\n",
      "steps = 0627 | overall loss = 0.00869 | time 68.343\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "batch_size = 7\n",
    "hidden_size = 128\n",
    "dtype = tf.float32\n",
    "\n",
    "with tf.variable_scope('fooo7', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [None, h, w, input_vec_size])\n",
    "    y = tf.placeholder(dtype, [None, h, w, output_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "    grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    epochs = 400\n",
    "    step = 0\n",
    "    for batch in batches(data, batch_size, epochs):\n",
    "        grad_step_start_time = time()\n",
    "\n",
    "        model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "            x: np.stack([x[0] for x in batch]),\n",
    "            y: np.stack([x[1] for x in batch]),\n",
    "        })\n",
    "\n",
    "        print('steps = {0} | overall loss = {1:.5f} | time {2:.3f}'.format(\n",
    "            str(step).zfill(4),\n",
    "            tot_loss_value,\n",
    "            time() - grad_step_start_time))\n",
    "\n",
    "        if tot_loss_value != tot_loss_value:\n",
    "            break\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)\n",
    "            \n",
    "        step += 1\n",
    "\n",
    "    saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Извод:** Не стигаме до нищо по-добро. Изглежда просто feature-ите, които си измислихме не са достатъчно добри. Ще опитаме още неща в нова тетрадка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
