{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import config\n",
    "from scenario import *\n",
    "from tileset import *\n",
    "from mdlstm import *\n",
    "\n",
    "from time import time\n",
    "\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека изчетем данните. Измежду тях ще изберем само карти за двама играчи, с най-малкия възможен размер (64 X 64), и използващи плочки тип `jungle`. Oчакваме картите да са 11 на брой. Бройката е доста малка, но ще разчитаме на [неразумната ефективност на рекурентните невронни мрежи](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios = []\n",
    "scenarios += process_scenarios(os.path.join(config.STARCRAFT_ROOT, 'Maps'))\n",
    "for directory in config.MAP_DIRECTORIES:\n",
    "    scenarios += process_scenarios(directory)\n",
    "\n",
    "scenarios = [x for x in scenarios if x.alliances == x.human_players == 2 and x.tileset == Tileset.JUNGLE and x.width == x.height == 64]\n",
    "\n",
    "len(scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ще имплементираме помощни функции, които извличат feature-и от плочка. Плочките силно зависят от околните. И може би има смисъл да приложим нещо подобно на word2vec.\n",
    "\n",
    "Но за сега ще направим малко feature engineering, за да изпробваме модела. Ще работим с 3 feature-а:\n",
    "- осреднена височина на плочката\n",
    "- коефициент за това върху каква част от плочката може да се ходи\n",
    "- дали върху плочката може да се строи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 64\n",
    "w = 64\n",
    "\n",
    "tile_vec_size = 3\n",
    "input_vec_size = 2 * tile_vec_size\n",
    "output_vec_size = tile_vec_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def to_features(tile):\n",
    "\n",
    "    @np.vectorize\n",
    "    def minitile_heights(minitile):\n",
    "        return minitile.height\n",
    "\n",
    "    @np.vectorize\n",
    "    def minitile_walkability(minitile):\n",
    "        return minitile.walkable\n",
    "\n",
    "    return np.array([\n",
    "        np.average(minitile_heights(tile.minitiles)),\n",
    "        np.average(minitile_walkability(tile.minitiles)),\n",
    "        tile.buildable\n",
    "    ], dtype=np.float32) + 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_features_by_index(tiles, horizontal_index, vertical_index):\n",
    "    if vertical_index >= 0 and horizontal_index >= 0:\n",
    "        tile = tiles[vertical_index, horizontal_index]\n",
    "        return to_features(tile)\n",
    "    else:\n",
    "        return np.zeros([tile_vec_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Експлицитни feature-и на всяка стъпка\n",
    "\n",
    "Всяка плочка ще зависи от горната и от лявата.\n",
    "\n",
    "В нашата двуизмерна рекурентна невронна мрежа ще използваме конкатенацията на feature-ите на горната и лявата плочки за вход. За изход ще използваме feature-ите на текущата плочка. За feature-и на плочки извън игралното поле ще връщаме списък нули.\n",
    "\n",
    "Всъщност скритият state за RNN-а също идва от горната и от лявата плочка. Така че на теория би трябвало да не е нужно да ги даваме експлицитно. Но при по-ранни експерименти имах **огромни** проблеми с числената стабилност на модела. За всякакъв смислено голям learning rate получавах NaN-ове в loss-a. Което всъщност не е изненада. Дори е описано в документацията на модела. Експлицитните feature-и са трик, който се справя с този проблем.\n",
    "\n",
    "Експлицитните feature-и на всяка стъпка имат още един плюс. Когато използваме модела за генерация, ще можем да избираме плочка различна от най-вероятната за всяка от стъпките."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    x = np.empty((h, w, input_vec_size), dtype=np.float32)\n",
    "    y = np.empty((h, w, output_vec_size), dtype=np.float32)\n",
    "    for vertical_index in range(h):\n",
    "        for horizontal_index in range(w):\n",
    "            top_tile_features = to_features_by_index(scenario.tiles, vertical_index - 1, horizontal_index)\n",
    "            left_tile_features = to_features_by_index(scenario.tiles, vertical_index, horizontal_index - 1)\n",
    "            x[vertical_index, horizontal_index, :] = np.concatenate([top_tile_features, left_tile_features])\n",
    "            y[vertical_index, horizontal_index, :] = to_features_by_index(scenario.tiles, vertical_index, horizontal_index)\n",
    "    data.append((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(data, batch_size, epochs):\n",
    "    all_batches = []\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(data)\n",
    "        all_batches += data\n",
    "\n",
    "    for i in range(0, epochs * len(data), batch_size):\n",
    "        inputs = all_batches[i: i + batch_size]\n",
    "        if len(inputs) == batch_size:\n",
    "            yield inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Терниране на модела\n",
    "\n",
    "Моделът идващ от модула `mdlstm`, както и кодът в тази тетрадка се базира на [tensorflow-multi-dimensional-lstm](https://github.com/philipperemy/tensorflow-multi-dimensional-lstm) написан от [Philippe Rémy](https://github.com/philipperemy) с разни промени от моя страна.\n",
    "\n",
    "Нека на базата на този код да имплементираме примерен трениращ код и да проверим дали моделът конвергира."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.24446\n",
      "steps = 0010 | overall loss = 0.18422\n",
      "steps = 0020 | overall loss = 0.15899\n",
      "steps = 0030 | overall loss = 0.16026\n",
      "steps = 0040 | overall loss = 0.14555\n",
      "steps = 0050 | overall loss = 0.14163\n",
      "steps = 0060 | overall loss = 0.14052\n",
      "steps = 0070 | overall loss = 0.13060\n",
      "steps = 0080 | overall loss = 0.11840\n",
      "steps = 0090 | overall loss = 0.10667\n",
      "steps = 0100 | overall loss = 0.09992\n",
      "steps = 0110 | overall loss = 0.09374\n",
      "steps = 0120 | overall loss = 0.08738\n",
      "steps = 0130 | overall loss = 0.08464\n",
      "steps = 0140 | overall loss = 0.07915\n",
      "steps = 0150 | overall loss = 0.07158\n",
      "steps = 0160 | overall loss = 0.06686\n",
      "steps = 0170 | overall loss = 0.06214\n",
      "steps = 0180 | overall loss = 0.05708\n",
      "steps = 0190 | overall loss = 0.05526\n",
      "steps = 0200 | overall loss = 0.05008\n",
      "steps = 0210 | overall loss = 0.04769\n",
      "steps = 0220 | overall loss = 0.04510\n",
      "steps = 0230 | overall loss = 0.04351\n",
      "steps = 0240 | overall loss = 0.04303\n",
      "steps = 0250 | overall loss = 0.03690\n",
      "steps = 0260 | overall loss = 0.03764\n",
      "steps = 0270 | overall loss = 0.03685\n",
      "steps = 0280 | overall loss = 0.03862\n",
      "steps = 0290 | overall loss = 0.03382\n",
      "steps = 0300 | overall loss = 0.03425\n",
      "steps = 0310 | overall loss = 0.03165\n",
      "steps = 0320 | overall loss = 0.03179\n",
      "steps = 0330 | overall loss = 0.03295\n",
      "steps = 0340 | overall loss = 0.03044\n",
      "steps = 0350 | overall loss = 0.02898\n",
      "steps = 0360 | overall loss = 0.03116\n",
      "steps = 0370 | overall loss = 0.02938\n",
      "steps = 0380 | overall loss = 0.02953\n",
      "steps = 0390 | overall loss = 0.02702\n",
      "steps = 0400 | overall loss = 0.02722\n",
      "steps = 0410 | overall loss = 0.02592\n",
      "steps = 0420 | overall loss = 0.03098\n",
      "steps = 0430 | overall loss = 0.02729\n",
      "steps = 0440 | overall loss = 0.02718\n",
      "steps = 0450 | overall loss = 0.02874\n",
      "steps = 0460 | overall loss = 0.02627\n",
      "steps = 0470 | overall loss = 0.02494\n",
      "steps = 0480 | overall loss = 0.02780\n",
      "steps = 0490 | overall loss = 0.02435\n",
      "steps = 0500 | overall loss = 0.02826\n",
      "steps = 0510 | overall loss = 0.02353\n",
      "steps = 0520 | overall loss = 0.02551\n",
      "steps = 0530 | overall loss = 0.02522\n",
      "steps = 0540 | overall loss = 0.01964\n",
      "steps = 0550 | overall loss = 0.02270\n",
      "steps = 0560 | overall loss = 0.02539\n",
      "steps = 0570 | overall loss = 0.02581\n",
      "steps = 0580 | overall loss = 0.02377\n",
      "steps = 0590 | overall loss = 0.02276\n",
      "steps = 0600 | overall loss = 0.02175\n",
      "steps = 0610 | overall loss = 0.02611\n",
      "steps = 0620 | overall loss = 0.02340\n",
      "steps = 0630 | overall loss = 0.02574\n",
      "steps = 0640 | overall loss = 0.02222\n",
      "steps = 0650 | overall loss = 0.02069\n",
      "steps = 0660 | overall loss = 0.02276\n",
      "steps = 0670 | overall loss = 0.02320\n",
      "steps = 0680 | overall loss = 0.02258\n",
      "steps = 0690 | overall loss = 0.02205\n",
      "steps = 0700 | overall loss = 0.02199\n",
      "steps = 0710 | overall loss = 0.01906\n",
      "steps = 0720 | overall loss = 0.01788\n",
      "steps = 0730 | overall loss = 0.02185\n",
      "steps = 0740 | overall loss = 0.02164\n",
      "steps = 0750 | overall loss = 0.02137\n",
      "steps = 0760 | overall loss = 0.02127\n",
      "steps = 0770 | overall loss = 0.02151\n",
      "steps = 0780 | overall loss = 0.02099\n",
      "steps = 0790 | overall loss = 0.02206\n",
      "steps = 0800 | overall loss = 0.01928\n",
      "steps = 0810 | overall loss = 0.01838\n",
      "steps = 0820 | overall loss = 0.01669\n",
      "steps = 0830 | overall loss = 0.02172\n",
      "steps = 0840 | overall loss = 0.01831\n",
      "steps = 0850 | overall loss = 0.02087\n",
      "steps = 0860 | overall loss = 0.01967\n",
      "steps = 0870 | overall loss = 0.01779\n",
      "steps = 0880 | overall loss = 0.02091\n",
      "steps = 0890 | overall loss = 0.02082\n",
      "steps = 0900 | overall loss = 0.02309\n",
      "steps = 0910 | overall loss = 0.02271\n",
      "steps = 0916 | overall loss = 0.020\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "batch_size = 6\n",
    "hidden_size = 16\n",
    "dtype = tf.float32\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "with tf.variable_scope('fooo4', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [batch_size, h, w, input_vec_size])\n",
    "    y = tf.placeholder(dtype, [batch_size, h, w, output_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "    grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    epochs = 500\n",
    "    step = 0\n",
    "    for batch in batches(data, batch_size, epochs):\n",
    "        grad_step_start_time = time()\n",
    "\n",
    "        model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "            x: np.stack([x[0] for x in batch]),\n",
    "            y: np.stack([x[1] for x in batch]),\n",
    "        })\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "\n",
    "        if tot_loss_value != tot_loss_value:\n",
    "            break\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print('steps = {0} | overall loss = {1:.3f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "    saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделът конвергира."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изследване на хиперпараметрите\n",
    "\n",
    "По-голям batch_size и learning_rate водят до NaN в стойностите за loss.\n",
    "По-малки стойности водят до по-бавно учене за всяка от итерациите.\n",
    "**TODO: да го демонстрирам**\n",
    "\n",
    "Освен това няма да даваме batch_size за x и y.\n",
    "Това ще направи полученият RNN модел една идея по-гъвкав за семплиране.\n",
    "Но това ще направи всяка от итерациите по-бавна.\n",
    "**TODO: да го демонстрирам**\n",
    "\n",
    "Интересен е hidden_size. По-високите му стойности водят до по-бавно учене в началото.\n",
    "Но след определен брой итерации резулттите са по-добри.\n",
    "Което не е изненада. И тъй като всяка итерация отнема повече време, ученето не е по-бързо като време.\n",
    "**TODO: да го демонстрирам**\n",
    "\n",
    "При предишните експерименти loss-ът не падаше много под 0.2.\n",
    "Въпросът е дали можем да достигнем по-добър краен резултат с по-голям hidden_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.21937\n",
      "steps = 0010 | overall loss = 0.15768\n",
      "steps = 0020 | overall loss = 0.15014\n",
      "steps = 0030 | overall loss = 0.13898\n",
      "steps = 0040 | overall loss = 0.13000\n",
      "steps = 0050 | overall loss = 0.12841\n",
      "steps = 0060 | overall loss = 0.11938\n",
      "steps = 0070 | overall loss = 0.10817\n",
      "steps = 0080 | overall loss = 0.10536\n",
      "steps = 0090 | overall loss = 0.10429\n",
      "steps = 0100 | overall loss = 0.10201\n",
      "steps = 0110 | overall loss = 0.09170\n",
      "steps = 0120 | overall loss = 0.08910\n",
      "steps = 0130 | overall loss = 0.08931\n",
      "steps = 0140 | overall loss = 0.08021\n",
      "steps = 0150 | overall loss = 0.07709\n",
      "steps = 0160 | overall loss = 0.07350\n",
      "steps = 0170 | overall loss = 0.07072\n",
      "steps = 0180 | overall loss = 0.06903\n",
      "steps = 0190 | overall loss = 0.06971\n",
      "steps = 0200 | overall loss = 0.06781\n",
      "steps = 0210 | overall loss = 0.06564\n",
      "steps = 0220 | overall loss = 0.06172\n",
      "steps = 0230 | overall loss = 0.06193\n",
      "steps = 0240 | overall loss = 0.05858\n",
      "steps = 0250 | overall loss = 0.05693\n",
      "steps = 0260 | overall loss = 0.05280\n",
      "steps = 0270 | overall loss = 0.05511\n",
      "steps = 0280 | overall loss = 0.05375\n",
      "steps = 0290 | overall loss = 0.05045\n",
      "steps = 0300 | overall loss = 0.05148\n",
      "steps = 0310 | overall loss = 0.04739\n",
      "steps = 0320 | overall loss = 0.04583\n",
      "steps = 0330 | overall loss = 0.04522\n",
      "steps = 0340 | overall loss = 0.04412\n",
      "steps = 0350 | overall loss = 0.04228\n",
      "steps = 0360 | overall loss = 0.04048\n",
      "steps = 0370 | overall loss = 0.03802\n",
      "steps = 0380 | overall loss = 0.03836\n",
      "steps = 0390 | overall loss = 0.03406\n",
      "steps = 0400 | overall loss = 0.03616\n",
      "steps = 0410 | overall loss = 0.03350\n",
      "steps = 0420 | overall loss = 0.03333\n",
      "steps = 0430 | overall loss = 0.03540\n",
      "steps = 0440 | overall loss = 0.03199\n",
      "steps = 0450 | overall loss = 0.02929\n",
      "steps = 0460 | overall loss = 0.03095\n",
      "steps = 0470 | overall loss = 0.03269\n",
      "steps = 0480 | overall loss = 0.03179\n",
      "steps = 0490 | overall loss = 0.02776\n",
      "steps = 0500 | overall loss = 0.03046\n",
      "steps = 0510 | overall loss = 0.02899\n",
      "steps = 0520 | overall loss = 0.02722\n",
      "steps = 0530 | overall loss = 0.02640\n",
      "steps = 0540 | overall loss = 0.02239\n",
      "steps = 0550 | overall loss = 0.02901\n",
      "steps = 0560 | overall loss = 0.02582\n",
      "steps = 0570 | overall loss = 0.02602\n",
      "steps = 0580 | overall loss = 0.02642\n",
      "steps = 0590 | overall loss = 0.02421\n",
      "steps = 0600 | overall loss = 0.02392\n",
      "steps = 0610 | overall loss = 0.02414\n",
      "steps = 0620 | overall loss = 0.02412\n",
      "steps = 0630 | overall loss = 0.01988\n",
      "steps = 0640 | overall loss = 0.02295\n",
      "steps = 0650 | overall loss = 0.02372\n",
      "steps = 0660 | overall loss = 0.02261\n",
      "steps = 0670 | overall loss = 0.02356\n",
      "steps = 0680 | overall loss = 0.02162\n",
      "steps = 0690 | overall loss = 0.02320\n",
      "steps = 0700 | overall loss = 0.01974\n",
      "steps = 0710 | overall loss = 0.02126\n",
      "steps = 0720 | overall loss = 0.02216\n",
      "steps = 0730 | overall loss = 0.02311\n",
      "steps = 0740 | overall loss = 0.02409\n",
      "steps = 0750 | overall loss = 0.02541\n",
      "steps = 0760 | overall loss = 0.02394\n",
      "steps = 0770 | overall loss = 0.02071\n",
      "steps = 0780 | overall loss = 0.02055\n",
      "steps = 0790 | overall loss = 0.01920\n",
      "steps = 0800 | overall loss = 0.02209\n",
      "steps = 0810 | overall loss = 0.01754\n",
      "steps = 0820 | overall loss = 0.01855\n",
      "steps = 0830 | overall loss = 0.02081\n",
      "steps = 0840 | overall loss = 0.02095\n",
      "steps = 0850 | overall loss = 0.02049\n",
      "steps = 0860 | overall loss = 0.02025\n",
      "steps = 0870 | overall loss = 0.02377\n",
      "steps = 0880 | overall loss = 0.01947\n",
      "steps = 0890 | overall loss = 0.01823\n",
      "steps = 0900 | overall loss = 0.01784\n",
      "steps = 0910 | overall loss = 0.02018\n",
      "steps = 0916 | overall loss = 0.02020\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "batch_size = 6\n",
    "hidden_size = 32\n",
    "dtype = tf.float32\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "with tf.variable_scope('fooo5', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [batch_size, h, w, input_vec_size])\n",
    "    y = tf.placeholder(dtype, [batch_size, h, w, output_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "    grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    epochs = 500\n",
    "    step = 0\n",
    "    for batch in batches(data, batch_size, epochs):\n",
    "        grad_step_start_time = time()\n",
    "\n",
    "        model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "            x: np.stack([x[0] for x in batch]),\n",
    "            y: np.stack([x[1] for x in batch]),\n",
    "        })\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "\n",
    "        if tot_loss_value != tot_loss_value:\n",
    "            break\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "    saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
