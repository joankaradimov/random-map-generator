{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import config\n",
    "from scenario import *\n",
    "from tileset import *\n",
    "from mdlstm import *\n",
    "\n",
    "from time import time\n",
    "\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека изчетем данните. Измежду тях ще изберем само карти за двама играчи, с най-малкия възможен размер (64 X 64), и използващи плочки тип `jungle`. Oчакваме картите да са 11 на брой. Бройката е доста малка, но ще разчитаме на [неразумната ефективност на рекурентните невронни мрежи](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios = []\n",
    "scenarios += process_scenarios(os.path.join(config.STARCRAFT_ROOT, 'Maps'))\n",
    "for directory in config.MAP_DIRECTORIES:\n",
    "    scenarios += process_scenarios(directory)\n",
    "\n",
    "scenarios = [x for x in scenarios if x.alliances == x.human_players == 2 and x.tileset == Tileset.JUNGLE and x.width == x.height == 64]\n",
    "\n",
    "len(scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ще имплементираме помощни функции, които извличат feature-и от плочка. Плочките силно зависят от околните. И може би има смисъл да приложим нещо подобно на word2vec.\n",
    "\n",
    "Но за сега ще направим малко feature engineering, за да изпробваме модела. Ще работим с 3 feature-а:\n",
    "- осреднена височина на плочката\n",
    "- коефициент за това върху каква част от плочката може да се ходи\n",
    "- дали върху плочката може да се строи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 64\n",
    "w = 64\n",
    "\n",
    "tile_vec_size = 3\n",
    "input_vec_size = 2 * tile_vec_size\n",
    "output_vec_size = tile_vec_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def to_features(tile):\n",
    "\n",
    "    @np.vectorize\n",
    "    def minitile_heights(minitile):\n",
    "        return minitile.height\n",
    "\n",
    "    @np.vectorize\n",
    "    def minitile_walkability(minitile):\n",
    "        return minitile.walkable\n",
    "\n",
    "    return np.array([\n",
    "        np.average(minitile_heights(tile.minitiles)),\n",
    "        np.average(minitile_walkability(tile.minitiles)),\n",
    "        tile.buildable\n",
    "    ], dtype=np.float32) + 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_features_by_index(tiles, horizontal_index, vertical_index):\n",
    "    if vertical_index >= 0 and horizontal_index >= 0:\n",
    "        tile = tiles[vertical_index, horizontal_index]\n",
    "        return to_features(tile)\n",
    "    else:\n",
    "        return np.zeros([tile_vec_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Експлицитни feature-и на всяка стъпка\n",
    "\n",
    "Всяка плочка ще зависи от горната и от лявата.\n",
    "\n",
    "В нашата двуизмерна рекурентна невронна мрежа ще използваме конкатенацията на feature-ите на горната и лявата плочки за вход. За изход ще използваме feature-ите на текущата плочка. За feature-и на плочки извън игралното поле ще връщаме списък нули.\n",
    "\n",
    "Всъщност скритият state за RNN-а също идва от горната и от лявата плочка. Така че на теория би трябвало да не е нужно да ги даваме експлицитно. Но при по-ранни експерименти имах **огромни** проблеми с числената стабилност на модела. За всякакъв смислено голям learning rate получавах NaN-ове в loss-a. Което всъщност не е изненада. Дори е описано в документацията на модела. Експлицитните feature-и са трик, който се справя с този проблем.\n",
    "\n",
    "Експлицитните feature-и на всяка стъпка имат още един плюс. Когато използваме модела за генерация, ще можем да избираме плочка различна от най-вероятната за всяка от стъпките."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    x = np.empty((h, w, input_vec_size), dtype=np.float32)\n",
    "    y = np.empty((h, w, output_vec_size), dtype=np.float32)\n",
    "    for vertical_index in range(h):\n",
    "        for horizontal_index in range(w):\n",
    "            top_tile_features = to_features_by_index(scenario.tiles, vertical_index - 1, horizontal_index)\n",
    "            left_tile_features = to_features_by_index(scenario.tiles, vertical_index, horizontal_index - 1)\n",
    "            x[vertical_index, horizontal_index, :] = np.concatenate([top_tile_features, left_tile_features])\n",
    "            y[vertical_index, horizontal_index, :] = to_features_by_index(scenario.tiles, vertical_index, horizontal_index)\n",
    "    data.append((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(data, batch_size, epochs):\n",
    "    all_batches = []\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(data)\n",
    "        all_batches += data\n",
    "\n",
    "    for i in range(0, epochs * len(data), batch_size):\n",
    "        inputs = all_batches[i: i + batch_size]\n",
    "        if len(inputs) == batch_size:\n",
    "            yield inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Терниране на модела\n",
    "\n",
    "Моделът идващ от модула `mdlstm`, както и кодът в тази тетрадка се базира на [tensorflow-multi-dimensional-lstm](https://github.com/philipperemy/tensorflow-multi-dimensional-lstm) написан от [Philippe Rémy](https://github.com/philipperemy) с разни промени от моя страна.\n",
    "\n",
    "Нека на базата на този код да имплементираме примерен трениращ код и да проверим дали моделът конвергира."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.24446\n",
      "steps = 0010 | overall loss = 0.18422\n",
      "steps = 0020 | overall loss = 0.15899\n",
      "steps = 0030 | overall loss = 0.16026\n",
      "steps = 0040 | overall loss = 0.14555\n",
      "steps = 0050 | overall loss = 0.14163\n",
      "steps = 0060 | overall loss = 0.14052\n",
      "steps = 0070 | overall loss = 0.13060\n",
      "steps = 0080 | overall loss = 0.11840\n",
      "steps = 0090 | overall loss = 0.10667\n",
      "steps = 0100 | overall loss = 0.09992\n",
      "steps = 0110 | overall loss = 0.09374\n",
      "steps = 0120 | overall loss = 0.08738\n",
      "steps = 0130 | overall loss = 0.08464\n",
      "steps = 0140 | overall loss = 0.07915\n",
      "steps = 0150 | overall loss = 0.07158\n",
      "steps = 0160 | overall loss = 0.06686\n",
      "steps = 0170 | overall loss = 0.06214\n",
      "steps = 0180 | overall loss = 0.05708\n",
      "steps = 0190 | overall loss = 0.05526\n",
      "steps = 0200 | overall loss = 0.05008\n",
      "steps = 0210 | overall loss = 0.04769\n",
      "steps = 0220 | overall loss = 0.04510\n",
      "steps = 0230 | overall loss = 0.04351\n",
      "steps = 0240 | overall loss = 0.04303\n",
      "steps = 0250 | overall loss = 0.03690\n",
      "steps = 0260 | overall loss = 0.03764\n",
      "steps = 0270 | overall loss = 0.03685\n",
      "steps = 0280 | overall loss = 0.03862\n",
      "steps = 0290 | overall loss = 0.03382\n",
      "steps = 0300 | overall loss = 0.03425\n",
      "steps = 0310 | overall loss = 0.03165\n",
      "steps = 0320 | overall loss = 0.03179\n",
      "steps = 0330 | overall loss = 0.03295\n",
      "steps = 0340 | overall loss = 0.03044\n",
      "steps = 0350 | overall loss = 0.02898\n",
      "steps = 0360 | overall loss = 0.03116\n",
      "steps = 0370 | overall loss = 0.02938\n",
      "steps = 0380 | overall loss = 0.02953\n",
      "steps = 0390 | overall loss = 0.02702\n",
      "steps = 0400 | overall loss = 0.02722\n",
      "steps = 0410 | overall loss = 0.02592\n",
      "steps = 0420 | overall loss = 0.03098\n",
      "steps = 0430 | overall loss = 0.02729\n",
      "steps = 0440 | overall loss = 0.02718\n",
      "steps = 0450 | overall loss = 0.02874\n",
      "steps = 0460 | overall loss = 0.02627\n",
      "steps = 0470 | overall loss = 0.02494\n",
      "steps = 0480 | overall loss = 0.02780\n",
      "steps = 0490 | overall loss = 0.02435\n",
      "steps = 0500 | overall loss = 0.02826\n",
      "steps = 0510 | overall loss = 0.02353\n",
      "steps = 0520 | overall loss = 0.02551\n",
      "steps = 0530 | overall loss = 0.02522\n",
      "steps = 0540 | overall loss = 0.01964\n",
      "steps = 0550 | overall loss = 0.02270\n",
      "steps = 0560 | overall loss = 0.02539\n",
      "steps = 0570 | overall loss = 0.02581\n",
      "steps = 0580 | overall loss = 0.02377\n",
      "steps = 0590 | overall loss = 0.02276\n",
      "steps = 0600 | overall loss = 0.02175\n",
      "steps = 0610 | overall loss = 0.02611\n",
      "steps = 0620 | overall loss = 0.02340\n",
      "steps = 0630 | overall loss = 0.02574\n",
      "steps = 0640 | overall loss = 0.02222\n",
      "steps = 0650 | overall loss = 0.02069\n",
      "steps = 0660 | overall loss = 0.02276\n",
      "steps = 0670 | overall loss = 0.02320\n",
      "steps = 0680 | overall loss = 0.02258\n",
      "steps = 0690 | overall loss = 0.02205\n",
      "steps = 0700 | overall loss = 0.02199\n",
      "steps = 0710 | overall loss = 0.01906\n",
      "steps = 0720 | overall loss = 0.01788\n",
      "steps = 0730 | overall loss = 0.02185\n",
      "steps = 0740 | overall loss = 0.02164\n",
      "steps = 0750 | overall loss = 0.02137\n",
      "steps = 0760 | overall loss = 0.02127\n",
      "steps = 0770 | overall loss = 0.02151\n",
      "steps = 0780 | overall loss = 0.02099\n",
      "steps = 0790 | overall loss = 0.02206\n",
      "steps = 0800 | overall loss = 0.01928\n",
      "steps = 0810 | overall loss = 0.01838\n",
      "steps = 0820 | overall loss = 0.01669\n",
      "steps = 0830 | overall loss = 0.02172\n",
      "steps = 0840 | overall loss = 0.01831\n",
      "steps = 0850 | overall loss = 0.02087\n",
      "steps = 0860 | overall loss = 0.01967\n",
      "steps = 0870 | overall loss = 0.01779\n",
      "steps = 0880 | overall loss = 0.02091\n",
      "steps = 0890 | overall loss = 0.02082\n",
      "steps = 0900 | overall loss = 0.02309\n",
      "steps = 0910 | overall loss = 0.02271\n",
      "steps = 0916 | overall loss = 0.020\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "batch_size = 6\n",
    "hidden_size = 16\n",
    "dtype = tf.float32\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "with tf.variable_scope('fooo4', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [batch_size, h, w, input_vec_size])\n",
    "    y = tf.placeholder(dtype, [batch_size, h, w, output_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "    grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    epochs = 500\n",
    "    step = 0\n",
    "    for batch in batches(data, batch_size, epochs):\n",
    "        grad_step_start_time = time()\n",
    "\n",
    "        model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "            x: np.stack([x[0] for x in batch]),\n",
    "            y: np.stack([x[1] for x in batch]),\n",
    "        })\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "\n",
    "        if tot_loss_value != tot_loss_value:\n",
    "            break\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print('steps = {0} | overall loss = {1:.3f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "    saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделът конвергира."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изследване на хиперпараметрите"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = tf.float32\n",
    "\n",
    "def train(learning_rate, batch_size, hidden_size, variable_scope):\n",
    "    with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
    "        x = tf.placeholder(dtype, [batch_size, h, w, input_vec_size])\n",
    "        y = tf.placeholder(dtype, [batch_size, h, w, output_vec_size])\n",
    "\n",
    "        mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "        rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "        model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "        grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        epochs = 10\n",
    "        step = 0\n",
    "        for batch in batches(data, batch_size, epochs):\n",
    "            grad_step_start_time = time()\n",
    "\n",
    "            model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "                x: np.stack([x[0] for x in batch]),\n",
    "                y: np.stack([x[1] for x in batch]),\n",
    "            })\n",
    "\n",
    "            print('steps = {0} | overall loss = {1:.5f} | time {2:.3f}'.format(str(step).zfill(4), tot_loss_value, time() - grad_step_start_time))\n",
    "\n",
    "            if tot_loss_value != tot_loss_value:\n",
    "                break\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нека разгледаме `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.22450 | time 7.404\n",
      "steps = 0001 | overall loss = 0.23928 | time 4.289\n",
      "steps = 0002 | overall loss = 0.19042 | time 4.279\n",
      "steps = 0003 | overall loss = 0.19908 | time 4.880\n",
      "steps = 0004 | overall loss = 0.18827 | time 4.365\n",
      "steps = 0005 | overall loss = 0.20849 | time 4.558\n",
      "steps = 0006 | overall loss = 0.19444 | time 4.335\n",
      "steps = 0007 | overall loss = 0.18227 | time 4.622\n",
      "steps = 0008 | overall loss = 0.16794 | time 5.012\n",
      "steps = 0009 | overall loss = 0.19071 | time 4.355\n",
      "steps = 0010 | overall loss = 0.18644 | time 4.441\n",
      "steps = 0011 | overall loss = 0.19184 | time 4.825\n",
      "steps = 0012 | overall loss = 0.16852 | time 4.884\n",
      "steps = 0013 | overall loss = 0.18219 | time 4.755\n",
      "steps = 0014 | overall loss = 0.18980 | time 4.537\n",
      "steps = 0015 | overall loss = 0.17211 | time 4.524\n",
      "steps = 0016 | overall loss = 0.17224 | time 4.737\n",
      "steps = 0017 | overall loss = 0.16989 | time 4.582\n",
      "steps = 0018 | overall loss = 0.17287 | time 4.516\n",
      "steps = 0019 | overall loss = 0.15418 | time 4.591\n",
      "steps = 0020 | overall loss = 0.17768 | time 4.634\n",
      "steps = 0021 | overall loss = 0.16572 | time 4.737\n",
      "steps = 0022 | overall loss = 0.16147 | time 4.758\n",
      "steps = 0023 | overall loss = 0.16892 | time 4.470\n",
      "steps = 0024 | overall loss = 0.15033 | time 4.731\n",
      "steps = 0025 | overall loss = 0.16455 | time 4.509\n",
      "steps = 0026 | overall loss = 0.16535 | time 4.583\n",
      "steps = 0027 | overall loss = 0.14471 | time 4.691\n",
      "steps = 0028 | overall loss = 0.15342 | time 4.493\n",
      "steps = 0029 | overall loss = 0.14424 | time 4.629\n",
      "steps = 0030 | overall loss = 0.16444 | time 4.633\n",
      "steps = 0031 | overall loss = 0.14894 | time 4.532\n",
      "steps = 0032 | overall loss = 0.15583 | time 4.643\n",
      "steps = 0033 | overall loss = 0.13949 | time 4.844\n",
      "steps = 0034 | overall loss = 0.14754 | time 5.020\n",
      "steps = 0035 | overall loss = 0.15638 | time 4.692\n",
      "steps = 0036 | overall loss = 0.14720 | time 4.732\n",
      "steps = 0037 | overall loss = 0.15685 | time 4.643\n",
      "steps = 0038 | overall loss = 0.15631 | time 4.706\n",
      "steps = 0039 | overall loss = 0.14372 | time 4.681\n",
      "steps = 0040 | overall loss = 0.14487 | time 4.595\n",
      "steps = 0041 | overall loss = 0.14576 | time 4.655\n",
      "steps = 0042 | overall loss = 0.14621 | time 4.620\n",
      "steps = 0043 | overall loss = 0.14158 | time 4.634\n",
      "steps = 0044 | overall loss = 0.13480 | time 4.623\n",
      "steps = 0045 | overall loss = 0.14239 | time 4.841\n",
      "steps = 0046 | overall loss = 0.14440 | time 4.679\n",
      "steps = 0047 | overall loss = 0.15449 | time 4.752\n",
      "steps = 0048 | overall loss = 0.13542 | time 4.838\n",
      "steps = 0049 | overall loss = 0.15792 | time 4.668\n",
      "steps = 0050 | overall loss = 0.14928 | time 4.723\n",
      "steps = 0051 | overall loss = 0.14227 | time 4.773\n",
      "steps = 0052 | overall loss = 0.13145 | time 4.608\n",
      "steps = 0053 | overall loss = 0.13768 | time 4.807\n",
      "steps = 0054 | overall loss = 0.13867 | time 5.341\n"
     ]
    }
   ],
   "source": [
    "train(1e-3, 2, 32, 'x1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.19480 | time 6.201\n",
      "steps = 0001 | overall loss = 0.23383 | time 3.631\n",
      "steps = 0002 | overall loss = 0.19265 | time 3.356\n",
      "steps = 0003 | overall loss = 0.20216 | time 3.388\n",
      "steps = 0004 | overall loss = 0.21528 | time 3.364\n",
      "steps = 0005 | overall loss = 0.18573 | time 3.399\n",
      "steps = 0006 | overall loss = 0.22116 | time 3.418\n",
      "steps = 0007 | overall loss = 0.19122 | time 3.432\n",
      "steps = 0008 | overall loss = 0.19894 | time 3.480\n",
      "steps = 0009 | overall loss = 0.19380 | time 3.802\n",
      "steps = 0010 | overall loss = 0.19283 | time 3.816\n",
      "steps = 0011 | overall loss = 0.17653 | time 4.121\n",
      "steps = 0012 | overall loss = 0.20932 | time 4.111\n",
      "steps = 0013 | overall loss = 0.18959 | time 4.088\n",
      "steps = 0014 | overall loss = 0.19014 | time 4.092\n",
      "steps = 0015 | overall loss = 0.19537 | time 4.136\n",
      "steps = 0016 | overall loss = 0.19537 | time 4.045\n",
      "steps = 0017 | overall loss = 0.19373 | time 4.183\n",
      "steps = 0018 | overall loss = 0.18951 | time 4.079\n",
      "steps = 0019 | overall loss = 0.18508 | time 4.004\n",
      "steps = 0020 | overall loss = 0.18357 | time 4.171\n",
      "steps = 0021 | overall loss = 0.18402 | time 4.030\n"
     ]
    }
   ],
   "source": [
    "train(1e-3, 5, 32, 'x1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = nan | time 7.857\n"
     ]
    }
   ],
   "source": [
    "train(1e-3, 8, 32, 'x1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Извод:** По-голям `batch_size` води до по-бързо обучение. Твърде голям води до числена нестабилност."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нека разгледаме `learning_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.23959 | time 8.347\n",
      "steps = 0001 | overall loss = 0.15093 | time 4.893\n",
      "steps = 0002 | overall loss = 0.16792 | time 4.884\n",
      "steps = 0003 | overall loss = 0.15135 | time 4.891\n",
      "steps = 0004 | overall loss = 0.16999 | time 5.683\n",
      "steps = 0005 | overall loss = 0.17150 | time 5.861\n",
      "steps = 0006 | overall loss = 0.17436 | time 4.892\n",
      "steps = 0007 | overall loss = 0.16512 | time 4.920\n",
      "steps = 0008 | overall loss = 0.17290 | time 4.968\n",
      "steps = 0009 | overall loss = 0.17663 | time 4.979\n",
      "steps = 0010 | overall loss = 0.16953 | time 5.051\n",
      "steps = 0011 | overall loss = 0.17352 | time 5.128\n",
      "steps = 0012 | overall loss = 0.16673 | time 5.161\n",
      "steps = 0013 | overall loss = 0.17824 | time 5.123\n",
      "steps = 0014 | overall loss = 0.17508 | time 5.201\n",
      "steps = 0015 | overall loss = 0.15553 | time 5.237\n",
      "steps = 0016 | overall loss = 0.17138 | time 5.228\n",
      "steps = 0017 | overall loss = 0.18042 | time 5.038\n",
      "steps = 0018 | overall loss = 0.18573 | time 5.076\n",
      "steps = 0019 | overall loss = 0.16574 | time 5.390\n",
      "steps = 0020 | overall loss = 0.16823 | time 5.021\n",
      "steps = 0021 | overall loss = 0.17202 | time 4.946\n"
     ]
    }
   ],
   "source": [
    "train(0.5, 5, 32, 'x1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.18755 | time 8.867\n",
      "steps = 0001 | overall loss = 0.17133 | time 5.844\n",
      "steps = 0002 | overall loss = 0.13229 | time 6.693\n",
      "steps = 0003 | overall loss = 0.12600 | time 5.389\n",
      "steps = 0004 | overall loss = 0.11812 | time 5.167\n",
      "steps = 0005 | overall loss = 0.11827 | time 4.847\n",
      "steps = 0006 | overall loss = 0.11511 | time 5.007\n",
      "steps = 0007 | overall loss = 0.10937 | time 4.914\n",
      "steps = 0008 | overall loss = 0.09374 | time 4.939\n",
      "steps = 0009 | overall loss = 0.08826 | time 4.983\n",
      "steps = 0010 | overall loss = 0.06434 | time 4.954\n",
      "steps = 0011 | overall loss = 0.06497 | time 4.984\n",
      "steps = 0012 | overall loss = 0.05683 | time 5.118\n",
      "steps = 0013 | overall loss = 0.05301 | time 4.972\n",
      "steps = 0014 | overall loss = 0.04811 | time 4.850\n",
      "steps = 0015 | overall loss = 0.05483 | time 4.984\n",
      "steps = 0016 | overall loss = 0.04312 | time 5.709\n",
      "steps = 0017 | overall loss = 0.05097 | time 5.023\n",
      "steps = 0018 | overall loss = 0.04086 | time 5.007\n",
      "steps = 0019 | overall loss = 0.03380 | time 4.974\n",
      "steps = 0020 | overall loss = 0.04399 | time 4.869\n",
      "steps = 0021 | overall loss = 0.03456 | time 5.090\n"
     ]
    }
   ],
   "source": [
    "train(0.1, 5, 32, 'x1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.24318 | time 8.537\n",
      "steps = 0001 | overall loss = 0.19397 | time 5.208\n",
      "steps = 0002 | overall loss = 0.23091 | time 5.577\n",
      "steps = 0003 | overall loss = 0.18486 | time 5.126\n",
      "steps = 0004 | overall loss = 0.18252 | time 5.016\n",
      "steps = 0005 | overall loss = 0.20282 | time 5.047\n",
      "steps = 0006 | overall loss = 0.18599 | time 5.028\n",
      "steps = 0007 | overall loss = 0.17421 | time 5.189\n",
      "steps = 0008 | overall loss = 0.18018 | time 5.024\n",
      "steps = 0009 | overall loss = 0.17664 | time 5.052\n",
      "steps = 0010 | overall loss = 0.16181 | time 5.108\n",
      "steps = 0011 | overall loss = 0.17543 | time 5.039\n",
      "steps = 0012 | overall loss = 0.15204 | time 5.059\n",
      "steps = 0013 | overall loss = 0.16106 | time 5.031\n",
      "steps = 0014 | overall loss = 0.15105 | time 5.068\n",
      "steps = 0015 | overall loss = 0.16661 | time 5.045\n",
      "steps = 0016 | overall loss = 0.14461 | time 5.023\n",
      "steps = 0017 | overall loss = 0.15033 | time 6.557\n",
      "steps = 0018 | overall loss = 0.14266 | time 5.014\n",
      "steps = 0019 | overall loss = 0.14872 | time 5.097\n",
      "steps = 0020 | overall loss = 0.15395 | time 5.067\n",
      "steps = 0021 | overall loss = 0.12903 | time 5.127\n"
     ]
    }
   ],
   "source": [
    "train(0.01, 5, 32, 'x1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Извод:** Изненадващо. При по-ранни експерименти всяка малко по-висока стойност на learning rate водеше до числена нестабилност. Възможно е да е било заради по-големият размер на картите (128x128 тогава, 64x64 сега). За сега ще оставим този въпрос отворен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нека разгледаме `hidden_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.26832 | time 7.288\n",
      "steps = 0001 | overall loss = 0.19507 | time 3.657\n",
      "steps = 0002 | overall loss = 0.21288 | time 3.855\n",
      "steps = 0003 | overall loss = 0.19213 | time 3.627\n",
      "steps = 0004 | overall loss = 0.18615 | time 3.628\n",
      "steps = 0005 | overall loss = 0.18491 | time 3.608\n",
      "steps = 0006 | overall loss = 0.18188 | time 3.668\n",
      "steps = 0007 | overall loss = 0.17009 | time 3.612\n",
      "steps = 0008 | overall loss = 0.18564 | time 3.593\n",
      "steps = 0009 | overall loss = 0.16213 | time 3.603\n",
      "steps = 0010 | overall loss = 0.17815 | time 3.651\n",
      "steps = 0011 | overall loss = 0.16675 | time 3.891\n",
      "steps = 0012 | overall loss = 0.17001 | time 3.712\n",
      "steps = 0013 | overall loss = 0.18198 | time 3.710\n",
      "steps = 0014 | overall loss = 0.15322 | time 3.710\n",
      "steps = 0015 | overall loss = 0.15681 | time 3.923\n",
      "steps = 0016 | overall loss = 0.17712 | time 4.003\n",
      "steps = 0017 | overall loss = 0.16453 | time 4.026\n",
      "steps = 0018 | overall loss = 0.15952 | time 4.041\n",
      "steps = 0019 | overall loss = 0.15602 | time 4.151\n",
      "steps = 0020 | overall loss = 0.15990 | time 4.325\n",
      "steps = 0021 | overall loss = 0.14829 | time 4.423\n"
     ]
    }
   ],
   "source": [
    "train(0.01, 5, 8, 'x5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.24190 | time 8.845\n",
      "steps = 0001 | overall loss = 0.19243 | time 5.362\n",
      "steps = 0002 | overall loss = 0.19146 | time 5.426\n",
      "steps = 0003 | overall loss = 0.19279 | time 5.388\n",
      "steps = 0004 | overall loss = 0.18755 | time 5.400\n",
      "steps = 0005 | overall loss = 0.17310 | time 5.512\n",
      "steps = 0006 | overall loss = 0.17268 | time 5.598\n",
      "steps = 0007 | overall loss = 0.16584 | time 5.426\n",
      "steps = 0008 | overall loss = 0.16395 | time 5.387\n",
      "steps = 0009 | overall loss = 0.16531 | time 5.583\n",
      "steps = 0010 | overall loss = 0.15367 | time 7.052\n",
      "steps = 0011 | overall loss = 0.15557 | time 5.301\n",
      "steps = 0012 | overall loss = 0.14952 | time 6.330\n",
      "steps = 0013 | overall loss = 0.16014 | time 5.378\n",
      "steps = 0014 | overall loss = 0.14689 | time 5.415\n",
      "steps = 0015 | overall loss = 0.14865 | time 5.512\n",
      "steps = 0016 | overall loss = 0.14717 | time 5.303\n",
      "steps = 0017 | overall loss = 0.14969 | time 5.448\n",
      "steps = 0018 | overall loss = 0.14041 | time 5.460\n",
      "steps = 0019 | overall loss = 0.14192 | time 5.828\n",
      "steps = 0020 | overall loss = 0.14870 | time 5.509\n",
      "steps = 0021 | overall loss = 0.13877 | time 5.430\n"
     ]
    }
   ],
   "source": [
    "train(0.01, 5, 32, 'x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.23936 | time 28.969\n",
      "steps = 0001 | overall loss = 0.18906 | time 25.882\n",
      "steps = 0002 | overall loss = 0.18992 | time 26.147\n",
      "steps = 0003 | overall loss = 0.18570 | time 26.899\n",
      "steps = 0004 | overall loss = 0.18603 | time 27.369\n",
      "steps = 0005 | overall loss = 0.16311 | time 28.521\n",
      "steps = 0006 | overall loss = 0.15637 | time 26.775\n",
      "steps = 0007 | overall loss = 0.15852 | time 26.869\n",
      "steps = 0008 | overall loss = 0.14762 | time 27.390\n",
      "steps = 0009 | overall loss = 0.14391 | time 26.984\n",
      "steps = 0010 | overall loss = 0.14746 | time 26.791\n",
      "steps = 0011 | overall loss = 0.14289 | time 26.477\n",
      "steps = 0012 | overall loss = 0.14387 | time 25.718\n",
      "steps = 0013 | overall loss = 0.12873 | time 25.919\n",
      "steps = 0014 | overall loss = 0.13779 | time 28.071\n",
      "steps = 0015 | overall loss = 0.14269 | time 28.430\n",
      "steps = 0016 | overall loss = 0.13338 | time 25.671\n",
      "steps = 0017 | overall loss = 0.13118 | time 29.361\n",
      "steps = 0018 | overall loss = 0.13405 | time 26.839\n",
      "steps = 0019 | overall loss = 0.12724 | time 26.530\n",
      "steps = 0020 | overall loss = 0.11983 | time 28.054\n",
      "steps = 0021 | overall loss = 0.12796 | time 26.105\n"
     ]
    }
   ],
   "source": [
    "train(0.01, 5, 128, 'x4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Извод:** Ефект има. Но е почти пренебрежим. За сметка на доста по-бавно време за трениране. С по-дълги тренирания може да видим по-смислена разлика. Нека пробваме по-дълго трениране..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.24157\n",
      "steps = 0010 | overall loss = 0.18574\n",
      "steps = 0020 | overall loss = 0.17465\n",
      "steps = 0030 | overall loss = 0.16504\n",
      "steps = 0040 | overall loss = 0.15868\n",
      "steps = 0050 | overall loss = 0.15054\n",
      "steps = 0060 | overall loss = 0.14977\n",
      "steps = 0070 | overall loss = 0.13328\n",
      "steps = 0080 | overall loss = 0.13253\n",
      "steps = 0090 | overall loss = 0.12998\n",
      "steps = 0100 | overall loss = 0.12211\n",
      "steps = 0110 | overall loss = 0.11872\n",
      "steps = 0120 | overall loss = 0.11888\n",
      "steps = 0130 | overall loss = 0.11038\n",
      "steps = 0140 | overall loss = 0.10638\n",
      "steps = 0150 | overall loss = 0.09887\n",
      "steps = 0160 | overall loss = 0.10004\n",
      "steps = 0170 | overall loss = 0.09228\n",
      "steps = 0180 | overall loss = 0.09305\n",
      "steps = 0190 | overall loss = 0.08365\n",
      "steps = 0200 | overall loss = 0.08484\n",
      "steps = 0210 | overall loss = 0.07976\n",
      "steps = 0220 | overall loss = 0.07927\n",
      "steps = 0230 | overall loss = 0.07513\n",
      "steps = 0240 | overall loss = 0.07296\n",
      "steps = 0250 | overall loss = 0.06869\n",
      "steps = 0260 | overall loss = 0.06693\n",
      "steps = 0270 | overall loss = 0.06637\n",
      "steps = 0280 | overall loss = 0.06620\n",
      "steps = 0290 | overall loss = 0.06114\n",
      "steps = 0300 | overall loss = 0.06247\n",
      "steps = 0310 | overall loss = 0.05903\n",
      "steps = 0320 | overall loss = 0.06056\n",
      "steps = 0330 | overall loss = 0.05701\n",
      "steps = 0340 | overall loss = 0.05646\n",
      "steps = 0350 | overall loss = 0.05634\n",
      "steps = 0360 | overall loss = 0.05480\n",
      "steps = 0370 | overall loss = 0.05430\n",
      "steps = 0380 | overall loss = 0.05113\n",
      "steps = 0390 | overall loss = 0.05114\n",
      "steps = 0400 | overall loss = 0.04569\n",
      "steps = 0410 | overall loss = 0.04823\n",
      "steps = 0420 | overall loss = 0.04778\n",
      "steps = 0430 | overall loss = 0.04454\n",
      "steps = 0440 | overall loss = 0.04515\n",
      "steps = 0450 | overall loss = 0.04354\n",
      "steps = 0460 | overall loss = 0.04522\n",
      "steps = 0470 | overall loss = 0.04327\n",
      "steps = 0480 | overall loss = 0.04177\n",
      "steps = 0490 | overall loss = 0.04246\n",
      "steps = 0500 | overall loss = 0.04064\n",
      "steps = 0510 | overall loss = 0.04006\n",
      "steps = 0520 | overall loss = 0.04045\n",
      "steps = 0530 | overall loss = 0.04170\n",
      "steps = 0540 | overall loss = 0.03435\n",
      "steps = 0550 | overall loss = 0.03882\n",
      "steps = 0560 | overall loss = 0.03697\n",
      "steps = 0570 | overall loss = 0.03358\n",
      "steps = 0580 | overall loss = 0.03600\n",
      "steps = 0590 | overall loss = 0.03612\n",
      "steps = 0600 | overall loss = 0.03083\n",
      "steps = 0610 | overall loss = 0.03185\n",
      "steps = 0620 | overall loss = 0.03242\n",
      "steps = 0630 | overall loss = 0.03280\n",
      "steps = 0640 | overall loss = 0.03213\n",
      "steps = 0650 | overall loss = 0.02987\n",
      "steps = 0660 | overall loss = 0.02767\n",
      "steps = 0670 | overall loss = 0.02927\n",
      "steps = 0680 | overall loss = 0.02697\n",
      "steps = 0690 | overall loss = 0.02667\n",
      "steps = 0700 | overall loss = 0.03024\n",
      "steps = 0710 | overall loss = 0.02541\n",
      "steps = 0720 | overall loss = 0.02851\n",
      "steps = 0730 | overall loss = 0.02528\n",
      "steps = 0740 | overall loss = 0.02565\n",
      "steps = 0750 | overall loss = 0.02448\n",
      "steps = 0760 | overall loss = 0.02642\n",
      "steps = 0770 | overall loss = 0.02594\n",
      "steps = 0780 | overall loss = 0.02448\n",
      "steps = 0785 | overall loss = 0.02642\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-3\n",
    "batch_size = 7\n",
    "hidden_size = 32\n",
    "dtype = tf.float32\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "with tf.variable_scope('fooo5', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [batch_size, h, w, input_vec_size])\n",
    "    y = tf.placeholder(dtype, [batch_size, h, w, output_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "    grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    epochs = 500\n",
    "    step = 0\n",
    "    for batch in batches(data, batch_size, epochs):\n",
    "        grad_step_start_time = time()\n",
    "\n",
    "        model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "            x: np.stack([x[0] for x in batch]),\n",
    "            y: np.stack([x[1] for x in batch]),\n",
    "        })\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "\n",
    "        if tot_loss_value != tot_loss_value:\n",
    "            break\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "    saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 0000 | overall loss = 0.21187\n",
      "steps = 0010 | overall loss = 0.17123\n",
      "steps = 0020 | overall loss = 0.15428\n",
      "steps = 0030 | overall loss = 0.14543\n",
      "steps = 0040 | overall loss = 0.13634\n",
      "steps = 0050 | overall loss = 0.12234\n",
      "steps = 0060 | overall loss = 0.10602\n",
      "steps = 0070 | overall loss = 0.10388\n",
      "steps = 0080 | overall loss = 0.09660\n",
      "steps = 0090 | overall loss = 0.08475\n",
      "steps = 0100 | overall loss = 0.07905\n",
      "steps = 0110 | overall loss = 0.07268\n",
      "steps = 0120 | overall loss = 0.06687\n",
      "steps = 0130 | overall loss = 0.06302\n",
      "steps = 0140 | overall loss = 0.05441\n",
      "steps = 0150 | overall loss = 0.05287\n",
      "steps = 0160 | overall loss = 0.05104\n",
      "steps = 0170 | overall loss = 0.04304\n",
      "steps = 0180 | overall loss = 0.04698\n",
      "steps = 0190 | overall loss = 0.04179\n",
      "steps = 0200 | overall loss = 0.03907\n",
      "steps = 0210 | overall loss = 0.03793\n",
      "steps = 0220 | overall loss = 0.03540\n",
      "steps = 0230 | overall loss = 0.03433\n",
      "steps = 0240 | overall loss = 0.03191\n",
      "steps = 0250 | overall loss = 0.03108\n",
      "steps = 0260 | overall loss = 0.02836\n",
      "steps = 0270 | overall loss = 0.02772\n",
      "steps = 0280 | overall loss = 0.02748\n",
      "steps = 0290 | overall loss = 0.02821\n",
      "steps = 0300 | overall loss = 0.02603\n",
      "steps = 0310 | overall loss = 0.02458\n",
      "steps = 0320 | overall loss = 0.02581\n",
      "steps = 0330 | overall loss = 0.02475\n",
      "steps = 0340 | overall loss = 0.02362\n",
      "steps = 0350 | overall loss = 0.02448\n",
      "steps = 0360 | overall loss = 0.02307\n",
      "steps = 0370 | overall loss = 0.02217\n",
      "steps = 0380 | overall loss = 0.02278\n",
      "steps = 0390 | overall loss = 0.02383\n",
      "steps = 0400 | overall loss = 0.02442\n",
      "steps = 0410 | overall loss = 0.02132\n",
      "steps = 0420 | overall loss = 0.02180\n",
      "steps = 0430 | overall loss = 0.02053\n",
      "steps = 0440 | overall loss = 0.01962\n",
      "steps = 0450 | overall loss = 0.02078\n",
      "steps = 0460 | overall loss = 0.02054\n",
      "steps = 0470 | overall loss = 0.02108\n",
      "steps = 0480 | overall loss = 0.02141\n",
      "steps = 0490 | overall loss = 0.02008\n",
      "steps = 0500 | overall loss = 0.02018\n",
      "steps = 0510 | overall loss = 0.01798\n",
      "steps = 0520 | overall loss = 0.01640\n",
      "steps = 0530 | overall loss = 0.01988\n",
      "steps = 0540 | overall loss = 0.02145\n",
      "steps = 0550 | overall loss = 0.01870\n",
      "steps = 0560 | overall loss = 0.01968\n",
      "steps = 0570 | overall loss = 0.01646\n",
      "steps = 0580 | overall loss = 0.01932\n",
      "steps = 0590 | overall loss = 0.01712\n",
      "steps = 0600 | overall loss = 0.01546\n",
      "steps = 0610 | overall loss = 0.01739\n",
      "steps = 0620 | overall loss = 0.01708\n",
      "steps = 0630 | overall loss = 0.01952\n",
      "steps = 0640 | overall loss = 0.01706\n",
      "steps = 0650 | overall loss = 0.01355\n",
      "steps = 0660 | overall loss = 0.01497\n",
      "steps = 0670 | overall loss = 0.01731\n",
      "steps = 0680 | overall loss = 0.01725\n",
      "steps = 0690 | overall loss = 0.01632\n",
      "steps = 0700 | overall loss = 0.01589\n",
      "steps = 0710 | overall loss = 0.01888\n",
      "steps = 0720 | overall loss = 0.01731\n",
      "steps = 0730 | overall loss = 0.01420\n",
      "steps = 0740 | overall loss = 0.01503\n",
      "steps = 0750 | overall loss = 0.01609\n",
      "steps = 0760 | overall loss = 0.01351\n",
      "steps = 0770 | overall loss = 0.01600\n",
      "steps = 0780 | overall loss = 0.01593\n",
      "steps = 0790 | overall loss = 0.01427\n",
      "steps = 0800 | overall loss = 0.01560\n",
      "steps = 0810 | overall loss = 0.01530\n",
      "steps = 0820 | overall loss = 0.01285\n",
      "steps = 0830 | overall loss = 0.01263\n",
      "steps = 0840 | overall loss = 0.01535\n",
      "steps = 0850 | overall loss = 0.01456\n",
      "steps = 0860 | overall loss = 0.01430\n",
      "steps = 0870 | overall loss = 0.01388\n",
      "steps = 0880 | overall loss = 0.01202\n",
      "steps = 0890 | overall loss = 0.01418\n",
      "steps = 0900 | overall loss = 0.01672\n",
      "steps = 0910 | overall loss = 0.01600\n",
      "steps = 0920 | overall loss = 0.01654\n",
      "steps = 0930 | overall loss = 0.01453\n",
      "steps = 0940 | overall loss = 0.01239\n",
      "steps = 0950 | overall loss = 0.01334\n",
      "steps = 0960 | overall loss = 0.01353\n",
      "steps = 0970 | overall loss = 0.01376\n",
      "steps = 0980 | overall loss = 0.01197\n",
      "steps = 0990 | overall loss = 0.01294\n",
      "steps = 1000 | overall loss = 0.01207\n",
      "steps = 1010 | overall loss = 0.01391\n",
      "steps = 1020 | overall loss = 0.01263\n",
      "steps = 1030 | overall loss = 0.01036\n",
      "steps = 1040 | overall loss = 0.01274\n",
      "steps = 1050 | overall loss = 0.01342\n",
      "steps = 1060 | overall loss = 0.01087\n",
      "steps = 1070 | overall loss = 0.01291\n",
      "steps = 1080 | overall loss = 0.01176\n",
      "steps = 1090 | overall loss = 0.01160\n",
      "steps = 1100 | overall loss = 0.01175\n",
      "steps = 1110 | overall loss = 0.01227\n",
      "steps = 1120 | overall loss = 0.01233\n",
      "steps = 1130 | overall loss = 0.01150\n",
      "steps = 1140 | overall loss = 0.01138\n",
      "steps = 1150 | overall loss = 0.01113\n",
      "steps = 1160 | overall loss = 0.01137\n",
      "steps = 1170 | overall loss = 0.01117\n",
      "steps = 1180 | overall loss = 0.01007\n",
      "steps = 1190 | overall loss = 0.01202\n",
      "steps = 1200 | overall loss = 0.01022\n",
      "steps = 1210 | overall loss = 0.01120\n",
      "steps = 1220 | overall loss = 0.01145\n",
      "steps = 1230 | overall loss = 0.01325\n",
      "steps = 1240 | overall loss = 0.01265\n",
      "steps = 1250 | overall loss = 0.01032\n",
      "steps = 1260 | overall loss = 0.01117\n",
      "steps = 1270 | overall loss = 0.00990\n",
      "steps = 1280 | overall loss = 0.00929\n",
      "steps = 1290 | overall loss = 0.01012\n",
      "steps = 1300 | overall loss = 0.01002\n",
      "steps = 1310 | overall loss = 0.01158\n",
      "steps = 1320 | overall loss = 0.01134\n",
      "steps = 1330 | overall loss = 0.01002\n",
      "steps = 1340 | overall loss = 0.00972\n",
      "steps = 1350 | overall loss = 0.00936\n",
      "steps = 1360 | overall loss = 0.01035\n",
      "steps = 1370 | overall loss = 0.00875\n",
      "steps = 1380 | overall loss = 0.01110\n",
      "steps = 1390 | overall loss = 0.01157\n",
      "steps = 1400 | overall loss = 0.01005\n",
      "steps = 1410 | overall loss = 0.01030\n",
      "steps = 1420 | overall loss = 0.01017\n",
      "steps = 1430 | overall loss = 0.01076\n",
      "steps = 1440 | overall loss = 0.01120\n",
      "steps = 1450 | overall loss = 0.00839\n",
      "steps = 1460 | overall loss = 0.00892\n",
      "steps = 1470 | overall loss = 0.00810\n",
      "steps = 1480 | overall loss = 0.00835\n",
      "steps = 1490 | overall loss = 0.00978\n",
      "steps = 1500 | overall loss = 0.00968\n",
      "steps = 1510 | overall loss = 0.00847\n",
      "steps = 1520 | overall loss = 0.00879\n",
      "steps = 1530 | overall loss = 0.00770\n",
      "steps = 1540 | overall loss = 0.00936\n",
      "steps = 1550 | overall loss = 0.00889\n",
      "steps = 1560 | overall loss = 0.00726\n",
      "steps = 1570 | overall loss = 0.00865\n",
      "steps = 1580 | overall loss = 0.00902\n",
      "steps = 1590 | overall loss = 0.00852\n",
      "steps = 1600 | overall loss = 0.00811\n",
      "steps = 1610 | overall loss = 0.00741\n",
      "steps = 1620 | overall loss = 0.00889\n",
      "steps = 1630 | overall loss = 0.00966\n",
      "steps = 1640 | overall loss = 0.00977\n",
      "steps = 1650 | overall loss = 0.00856\n",
      "steps = 1660 | overall loss = 0.01012\n",
      "steps = 1670 | overall loss = 0.00871\n",
      "steps = 1680 | overall loss = 0.00915\n",
      "steps = 1690 | overall loss = 0.00774\n",
      "steps = 1700 | overall loss = 0.00967\n",
      "steps = 1710 | overall loss = 0.00884\n",
      "steps = 1720 | overall loss = 0.00948\n",
      "steps = 1730 | overall loss = 0.00762\n",
      "steps = 1740 | overall loss = 0.00834\n",
      "steps = 1750 | overall loss = 0.00954\n",
      "steps = 1760 | overall loss = 0.00866\n",
      "steps = 1770 | overall loss = 0.00709\n",
      "steps = 1780 | overall loss = 0.00755\n",
      "steps = 1790 | overall loss = 0.00788\n",
      "steps = 1800 | overall loss = 0.00845\n",
      "steps = 1810 | overall loss = 0.00891\n",
      "steps = 1820 | overall loss = 0.00791\n",
      "steps = 1830 | overall loss = 0.00830\n",
      "steps = 1840 | overall loss = 0.00762\n",
      "steps = 1850 | overall loss = 0.00873\n",
      "steps = 1860 | overall loss = 0.00806\n",
      "steps = 1870 | overall loss = 0.00766\n",
      "steps = 1880 | overall loss = 0.00780\n",
      "steps = 1890 | overall loss = 0.00697\n",
      "steps = 1900 | overall loss = 0.00680\n",
      "steps = 1910 | overall loss = 0.00894\n",
      "steps = 1920 | overall loss = 0.00820\n",
      "steps = 1930 | overall loss = 0.00687\n",
      "steps = 1940 | overall loss = 0.00813\n",
      "steps = 1950 | overall loss = 0.00908\n",
      "steps = 1960 | overall loss = 0.00799\n",
      "steps = 1970 | overall loss = 0.00740\n",
      "steps = 1980 | overall loss = 0.00782\n",
      "steps = 1990 | overall loss = 0.00676\n",
      "steps = 2000 | overall loss = 0.00737\n",
      "steps = 2010 | overall loss = 0.00718\n",
      "steps = 2020 | overall loss = 0.00690\n",
      "steps = 2030 | overall loss = 0.00781\n",
      "steps = 2040 | overall loss = 0.00909\n",
      "steps = 2050 | overall loss = 0.00878\n",
      "steps = 2060 | overall loss = 0.00717\n",
      "steps = 2070 | overall loss = 0.00633\n",
      "steps = 2080 | overall loss = 0.00646\n",
      "steps = 2090 | overall loss = 0.00637\n",
      "steps = 2100 | overall loss = 0.00774\n",
      "steps = 2110 | overall loss = 0.00809\n",
      "steps = 2120 | overall loss = 0.00705\n",
      "steps = 2130 | overall loss = 0.00694\n",
      "steps = 2140 | overall loss = 0.00756\n",
      "steps = 2150 | overall loss = 0.00716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 2160 | overall loss = 0.00679\n",
      "steps = 2170 | overall loss = 0.00659\n",
      "steps = 2180 | overall loss = 0.00711\n",
      "steps = 2190 | overall loss = 0.00589\n",
      "steps = 2200 | overall loss = 0.00651\n",
      "steps = 2210 | overall loss = 0.00888\n",
      "steps = 2220 | overall loss = 0.00951\n",
      "steps = 2230 | overall loss = 0.00762\n",
      "steps = 2240 | overall loss = 0.00665\n",
      "steps = 2250 | overall loss = 0.00823\n",
      "steps = 2260 | overall loss = 0.00601\n",
      "steps = 2270 | overall loss = 0.00690\n",
      "steps = 2280 | overall loss = 0.00599\n",
      "steps = 2290 | overall loss = 0.00608\n",
      "steps = 2300 | overall loss = 0.00624\n",
      "steps = 2310 | overall loss = 0.00658\n",
      "steps = 2320 | overall loss = 0.00798\n",
      "steps = 2330 | overall loss = 0.00728\n",
      "steps = 2340 | overall loss = 0.00623\n",
      "steps = 2350 | overall loss = 0.00629\n",
      "steps = 2360 | overall loss = 0.00529\n",
      "steps = 2370 | overall loss = 0.00715\n",
      "steps = 2380 | overall loss = 0.00633\n",
      "steps = 2390 | overall loss = 0.00761\n",
      "steps = 2400 | overall loss = 0.00662\n",
      "steps = 2410 | overall loss = 0.00717\n",
      "steps = 2420 | overall loss = 0.00761\n",
      "steps = 2430 | overall loss = 0.00763\n",
      "steps = 2440 | overall loss = 0.00682\n",
      "steps = 2450 | overall loss = 0.00707\n",
      "steps = 2460 | overall loss = 0.00696\n",
      "steps = 2470 | overall loss = 0.00542\n",
      "steps = 2480 | overall loss = 0.00618\n",
      "steps = 2490 | overall loss = 0.00649\n",
      "steps = 2500 | overall loss = 0.00583\n",
      "steps = 2510 | overall loss = 0.00694\n",
      "steps = 2520 | overall loss = 0.00552\n",
      "steps = 2530 | overall loss = 0.00600\n",
      "steps = 2540 | overall loss = 0.00597\n",
      "steps = 2550 | overall loss = 0.00640\n",
      "steps = 2560 | overall loss = 0.00505\n",
      "steps = 2570 | overall loss = 0.00542\n",
      "steps = 2580 | overall loss = 0.00526\n",
      "steps = 2590 | overall loss = 0.00867\n",
      "steps = 2600 | overall loss = 0.00802\n",
      "steps = 2610 | overall loss = 0.00721\n",
      "steps = 2620 | overall loss = 0.00622\n",
      "steps = 2630 | overall loss = 0.00605\n",
      "steps = 2640 | overall loss = 0.00575\n",
      "steps = 2650 | overall loss = 0.00593\n",
      "steps = 2660 | overall loss = 0.00532\n",
      "steps = 2670 | overall loss = 0.00608\n",
      "steps = 2680 | overall loss = 0.00524\n",
      "steps = 2690 | overall loss = 0.00445\n",
      "steps = 2700 | overall loss = 0.00576\n",
      "steps = 2710 | overall loss = 0.00717\n",
      "steps = 2720 | overall loss = 0.00609\n",
      "steps = 2730 | overall loss = 0.00528\n",
      "steps = 2740 | overall loss = 0.00557\n",
      "steps = 2750 | overall loss = 0.00517\n",
      "steps = 2760 | overall loss = 0.00473\n",
      "steps = 2770 | overall loss = 0.00697\n",
      "steps = 2780 | overall loss = 0.00680\n",
      "steps = 2790 | overall loss = 0.00599\n",
      "steps = 2800 | overall loss = 0.00694\n",
      "steps = 2810 | overall loss = 0.00704\n",
      "steps = 2820 | overall loss = 0.00619\n",
      "steps = 2830 | overall loss = 0.00630\n",
      "steps = 2840 | overall loss = 0.00593\n",
      "steps = 2850 | overall loss = 0.00492\n",
      "steps = 2860 | overall loss = 0.00465\n",
      "steps = 2870 | overall loss = 0.00507\n",
      "steps = 2880 | overall loss = 0.00464\n",
      "steps = 2890 | overall loss = 0.00474\n",
      "steps = 2900 | overall loss = 0.00489\n",
      "steps = 2910 | overall loss = 0.00527\n",
      "steps = 2920 | overall loss = 0.00540\n",
      "steps = 2930 | overall loss = 0.00579\n",
      "steps = 2940 | overall loss = 0.00477\n",
      "steps = 2950 | overall loss = 0.00510\n",
      "steps = 2960 | overall loss = 0.00626\n",
      "steps = 2970 | overall loss = 0.00586\n",
      "steps = 2980 | overall loss = 0.00643\n",
      "steps = 2990 | overall loss = 0.00605\n",
      "steps = 3000 | overall loss = 0.00569\n",
      "steps = 3010 | overall loss = 0.00490\n",
      "steps = 3020 | overall loss = 0.00542\n",
      "steps = 3030 | overall loss = 0.00446\n",
      "steps = 3040 | overall loss = 0.00456\n",
      "steps = 3050 | overall loss = 0.00479\n",
      "steps = 3060 | overall loss = 0.00654\n",
      "steps = 3070 | overall loss = 0.00598\n",
      "steps = 3080 | overall loss = 0.00584\n",
      "steps = 3090 | overall loss = 0.00742\n",
      "steps = 3100 | overall loss = 0.00679\n",
      "steps = 3110 | overall loss = 0.00640\n",
      "steps = 3120 | overall loss = 0.00528\n",
      "steps = 3130 | overall loss = 0.00498\n",
      "steps = 3140 | overall loss = 0.00437\n",
      "steps = 3142 | overall loss = 0.00552\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-3\n",
    "batch_size = 7\n",
    "hidden_size = 64\n",
    "dtype = tf.float32\n",
    "\n",
    "with tf.variable_scope('fooo6', reuse=tf.AUTO_REUSE):\n",
    "    x = tf.placeholder(dtype, [None, h, w, input_vec_size])\n",
    "    y = tf.placeholder(dtype, [None, h, w, output_vec_size])\n",
    "\n",
    "    mdrnn_while_loop = MdRnnWhileLoop(dtype)\n",
    "    rnn_out, _ = mdrnn_while_loop(rnn_size=hidden_size, input_data=x)\n",
    "    model_out = slim.fully_connected(inputs=rnn_out, num_outputs=output_vec_size, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - model_out))\n",
    "    grad_update = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    epochs = 2000\n",
    "    step = 0\n",
    "    for batch in batches(data, batch_size, epochs):\n",
    "        model_preds, tot_loss_value, _ = sess.run([model_out, loss, grad_update], feed_dict={\n",
    "            x: np.stack([x[0] for x in batch]),\n",
    "            y: np.stack([x[1] for x in batch]),\n",
    "        })\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "\n",
    "        if tot_loss_value != tot_loss_value:\n",
    "            break\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print('steps = {0} | overall loss = {1:.5f}'.format(str(step).zfill(4), tot_loss_value))\n",
    "    saver.save(sess, os.path.join('checkpoints', 'model'), global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На стъпка 780 виждаме, че от по-голям размер на hidden state има доста смисъл. Оставихме второто трениране да мине през повече епохи след стъпка 780 - но това е с друга цел.\n",
    "\n",
    "**Извод:** След достатъчно голям брой епохи започва да се вижда смисъл от `hidden_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
